{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find duplicate questions on StackOverflow by their embeddings\n",
    "\n",
    "In this assignment you will learn how to calculate a similarity for pieces of text. Using this approach you will know how to find duplicate questions from [StackOverflow](https://stackoverflow.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "In this task you will you will need the following libraries:\n",
    "- [StarSpace](https://github.com/facebookresearch/StarSpace) — a general-purpose model for efficient learning of entity embeddings from Facebook\n",
    "- [Gensim](https://radimrehurek.com/gensim/) — a tool for solving various NLP-related tasks (topic modeling, text representation, ...)\n",
    "- [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    "- [Nltk](http://www.nltk.org) — a platform to work with human language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The following cell will download all data required for this assignment into the folder `week3/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/train.tsv is already downloaded.\n",
      "File data/validation.tsv is already downloaded.\n",
      "File data/test.tsv is already downloaded.\n",
      "File data/test_embeddings.tsv is already downloaded.\n",
      "Downloading GoogleNews-vectors-negative300.bin.gz (1.5G) for you, it will take a while...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bdeeea1bf84488fa44cd01f505cdf69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1647046227), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.download_utils import download_week3_resources\n",
    "\n",
    "download_week3_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading\n",
    "We will create a grader instace below and use it to collect your answers. Note that these outputs will be stored locally inside grader and will be uploaded to platform only after running submiting function in the last part of this assignment. If you want to make partial submission, you can run that cell any time you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grader import Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader = Grader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding\n",
    "\n",
    "To solve the problem, you will use two different models of embeddings:\n",
    "\n",
    " - [Pre-trained word vectors](https://code.google.com/archive/p/word2vec/) from Google which were trained on a part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. `GoogleNews-vectors-negative300.bin.gz` will be downloaded in `download_week3_resources()`.\n",
    " - Representations using StarSpace on StackOverflow data sample. You will need to train them from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always easier to start with pre-trained embeddings. Unpack the pre-trained Goggle's vectors and upload them using the function [KeyedVectors.load_word2vec_format](https://radimrehurek.com/gensim/models/keyedvectors.html) from gensim library with the parameter *binary=True*. If the size of the embeddings is larger than the avaliable memory, you could load only a part of the embeddings by defining the parameter *limit* (recommended: 500000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_embeddings = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',binary=True,limit =20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to work with Google's word2vec embeddings?\n",
    "\n",
    "Once you have loaded the representations, make sure you can access them. First, you can check if the loaded embeddings contain a word:\n",
    "    \n",
    "    'word' in wv_embeddings\n",
    "    \n",
    "Second, to get the corresponding embedding you can use the square brackets:\n",
    "\n",
    "    wv_embeddings['word']\n",
    " \n",
    "### Checking that the embeddings are correct \n",
    " \n",
    "To prevent any errors during the first stage, we can check that the loaded embeddings are correct. You can call the function *check_embeddings*, implemented below, which runs 3 tests:\n",
    "1. Find the most similar word for provided \"positive\" and \"negative\" words.\n",
    "2. Find which word from the given list doesn’t go with the others.\n",
    "3. Find the most similar word for the provided one.\n",
    "\n",
    "In the right case the function will return the string *These embeddings look good*. Othervise, you need to validate the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_embeddings(embeddings):\n",
    "    error_text = \"Something wrong with your embeddings ('%s test isn't correct).\"\n",
    "    most_similar = embeddings.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "    if len(most_similar) < 1 or most_similar[0][0] != 'queen':\n",
    "        return error_text % \"Most similar\"\n",
    "\n",
    "    doesnt_match = embeddings.doesnt_match(['breakfast', 'cereal', 'dinner', 'lunch'])\n",
    "    if doesnt_match != 'cereal':\n",
    "        return error_text % \"Doesn't match\"\n",
    "    \n",
    "    most_similar_to_given = embeddings.most_similar_to_given('music', ['water', 'sound', 'backpack', 'mouse'])\n",
    "    if most_similar_to_given != 'sound':\n",
    "        return error_text % \"Most similar to given\"\n",
    "    \n",
    "    return \"These embeddings look good.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These embeddings look good.\n"
     ]
    }
   ],
   "source": [
    "print(check_embeddings(wv_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From word to text embeddings\n",
    "\n",
    "**Task 1 (Question2Vec).** Usually, we have word-based embeddings, but for the task we need to create a representation for the whole question. It could be done in different ways. In our case we will use a **mean** of all word vectors in the question. Now you need to implement the function *question_to_vec*, which calculates the question representation described above. This function should work with the input text as is without any preprocessing.\n",
    "\n",
    "Note that there could be words without the corresponding embeddings. In this case, you can just skip these words and don't take them into account during calculating the result. If the question doesn't contain any known word with embedding, the function should return a zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec(question, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        embeddings: dict where the key is a word and a value is its' embedding\n",
    "        dim: size of the representation\n",
    "\n",
    "        result: vector representation for the question\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    vec = np.zeros(dim)\n",
    "    counter = 0\n",
    "    if not question:\n",
    "        return np.zeros(dim)\n",
    "    \n",
    "    for i in question.split():\n",
    "        if i in embeddings:\n",
    "            vec += embeddings[i]\n",
    "            counter += 1\n",
    "    if not np.any(vec):\n",
    "        return np.zeros(dim)\n",
    "    return vec/counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the basic correctness of your implementation, run the function *question_to_vec_tests*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec_tests():\n",
    "    if (np.zeros(300) != question_to_vec('', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for empty question.\"\n",
    "    if (np.zeros(300) != question_to_vec('thereisnosuchword', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for the question, which consists only unknown words.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('word', wv_embeddings)).any():\n",
    "        return \"You need to check the corectness of your function.\"\n",
    "    if ((wv_embeddings['I'] + wv_embeddings['am']) / 2 != question_to_vec('I am', wv_embeddings)).any():\n",
    "        return \"Your function should calculate a mean of word vectors.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('thereisnosuchword word', wv_embeddings)).any():\n",
    "        return \"You should not consider words which embeddings are unknown.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(question_to_vec_tests())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can submit embeddings for the questions from the file *test_embeddings.tsv* to earn the points. In this task you don't need to transform the text of a question somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from util import array_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task Question2Vec is: 0.0236104329427\n",
      "-0.0376790364583\n",
      "-0.000414530436198\n",
      "0.0884195963542\n",
      "0.0208333333333\n",
      "-0.0496622721354...\n"
     ]
    }
   ],
   "source": [
    "question2vec_result = []\n",
    "for question in open('data/test_embeddings.tsv'):\n",
    "    question = question.strip()\n",
    "    answer = question_to_vec(question, wv_embeddings)\n",
    "    question2vec_result = np.append(question2vec_result, answer)\n",
    "\n",
    "grader.submit_tag('Question2Vec', array_to_string(question2vec_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a method to create a representation of any sentence and we are ready for the first evaluation. So, let's check how well our solution (Google's vectors + *question_to_vec*) will work.\n",
    "\n",
    "## Evaluation of text similarity\n",
    "\n",
    "We can imagine that if we use good embeddings, the cosine similarity between the duplicate sentences should be less than for the random ones. Overall, for each pair of duplicate sentences we can generate *R* random negative examples and find out the position of the correct duplicate.  \n",
    "\n",
    "For example, we have the question *\"Exceptions What really happens\"* and we are sure that another question *\"How does the catch keyword determine the type of exception that was thrown\"* is a duplicate. But our model doesn't know it and tries to find out the best option also among questions like *\"How Can I Make These Links Rotate in PHP\"*, *\"NSLog array description not memory address\"* and *\"PECL_HTTP not recognised php ubuntu\"*. The goal of the model is to rank all these 4 questions (1 *positive* and *R* = 3 *negative*) in the way that the correct one is in the first place.\n",
    "\n",
    "However, it is unnatural to count on that the best candidate will be always in the first place. So let us consider the place of the best candidate in the sorted list of candidates and formulate a metric based on it. We can fix some *K* — a reasonalble number of top-ranked elements and *N* — a number of queries (size of the sample).\n",
    "\n",
    "### Hits@K\n",
    "\n",
    "The first simple metric will be a number of correct hits for some *K*:\n",
    "$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [dup_i \\in topK(q_i)]$$\n",
    "\n",
    "where $q_i$ is the i-th query, $dup_i$ is its duplicate, $topK(q_i)$ is the top K elements of the ranked sentences provided by our model and the operation $[dup_i \\in topK(q_i)]$ equals 1 if the condition is true and 0 otherwise (more details about this operation could be found [here](https://en.wikipedia.org/wiki/Iverson_bracket)).\n",
    "\n",
    "\n",
    "### DCG@K\n",
    "The second one is a simplified [DCG metric](https://en.wikipedia.org/wiki/Discounted_cumulative_gain):\n",
    "\n",
    "$$ \\text{DCG@K} = \\frac{1}{N} \\sum_{i=1}^N\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le K] $$\n",
    "\n",
    "where $rank_{dup_i}$ is a position of the duplicate in the sorted list of the nearest sentences for the query $q_i$. According to this metric, the model gets a higher reward for a higher position of the correct answer. If the answer does not appear in topK at all, the reward is zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation examples\n",
    "\n",
    "Let's calculate the described metrics for the toy example introduced above. In this case $N$ = 1 and the correct candidate for $q_1$ is *\"How does the catch keyword determine the type of exception that was thrown\"*. Consider the following ranking of the candidates:\n",
    "1. *\"How Can I Make These Links Rotate in PHP\"*\n",
    "2. *\"How does the catch keyword determine the type of exception that was thrown\"*\n",
    "3. *\"NSLog array description not memory address\"*\n",
    "4. *\"PECL_HTTP not recognised php ubuntu\"*\n",
    "\n",
    "Using the ranking above, calculate *Hits@K* metric for *K = 1, 2, 4*: \n",
    " \n",
    "- [K = 1] $\\text{Hits@1} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top1(q_i)] = [dup_1 \\in top1(q_1)] = 0$ because the correct answer doesn't appear in the *top1* list.\n",
    "- [K = 2] $\\text{Hits@2} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top2(q_i)] = [dup_1 \\in top2(q_1)] = 1$ because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{Hits@4} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top4(q_i)] = [dup_1 \\in top4(q_1)] = 1$\n",
    "\n",
    "Using the ranking above, calculate *DCG@K* metric for *K = 1, 2, 4*:\n",
    "\n",
    "- [K = 1] $\\text{DCG@1} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = \\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = 0$ because the correct answer doesn't appear in the top1 list.\n",
    "- [K = 2] $\\text{DCG@2} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 2] = \\frac{1}{\\log_2{3}}$, because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{DCG@4} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 4] = \\frac{1}{\\log_2{3}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks 2 and 3 (HitsCount and DCGScore).** Implement the functions *hits_count* and *dcg_score* as described above. Each function has two arguments: *dup_ranks* and *k*. *dup_ranks* is a list which contains *values of ranks* of duplicates. For example, *dup_ranks* is *[2]* for the example provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_count(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question; \n",
    "                   length is a number of questions which we are looking for duplicates; \n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in Hits@k metric)\n",
    "\n",
    "        result: return Hits@k value for current ranking\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    N= len(dup_ranks)\n",
    "    if not N:\n",
    "        return 0\n",
    "    s = 0 \n",
    "    for i in range(N):\n",
    "        if dup_ranks[i]<=k:\n",
    "            s += 1\n",
    "    return s/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hits():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1, 1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "    \n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "\n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_hits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question; \n",
    "                   length is a number of questions which we are looking for duplicates; \n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in DCG@k metric)\n",
    "\n",
    "        result: return DCG@k value for current ranking\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    \n",
    "    N=len(dup_ranks)\n",
    "    if not N:\n",
    "        return 0\n",
    "    s=0\n",
    "    for i in range(N):\n",
    "        if dup_ranks[i]<=k:\n",
    "            s+=1/np.log2(1+dup_ranks[i])\n",
    "    return s/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dcg():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1 / (np.log2(3)), 1 / (np.log2(3)), 1 / (np.log2(3))]\n",
    "    \n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "\n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, (1 + (1 / (np.log2(3)))) / 2]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "        \n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\",\n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_dcg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit results of the functions *hits_count* and *dcg_score* for the following examples to earn the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    [1],\n",
    "    [1, 2],\n",
    "    [2, 1],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    [9, 5, 4, 2, 8, 10, 7, 6, 1, 3],\n",
    "    [4, 3, 5, 1, 9, 10, 7, 8, 2, 6],\n",
    "    [5, 1, 7, 6, 2, 3, 8, 9, 10, 4],\n",
    "    [6, 3, 1, 4, 7, 2, 9, 8, 10, 5],\n",
    "    [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task HitsCount is: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1....\n"
     ]
    }
   ],
   "source": [
    "hits_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        hits_results.append(hits_count(example, k + 1))\n",
    "grader.submit_tag('HitsCount', array_to_string(hits_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task DCGScore is: 1.0\n",
      "0.5\n",
      "0.815464876786\n",
      "0.5\n",
      "0.815464876786\n",
      "0.333333333333\n",
      "0.54364325119\n",
      "0.710309917857\n",
      "0.1\n",
      "0.16309297...\n"
     ]
    }
   ],
   "source": [
    "dcg_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        dcg_results.append(dcg_score(example, k + 1))\n",
    "grader.submit_tag('DCGScore', array_to_string(dcg_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  First solution: pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with predefined train, validation and test corpora. All the files are tab-separated, but have a different format:\n",
    " - *train* corpus contains similar sentences at the same row.\n",
    " - *validation* corpus contains the following columns: *question*, *similar question*, *negative example 1*, *negative example 2*, ... \n",
    " - *test* corpus contains the following columns: *question*, *example 1*, *example 2*, ...\n",
    "\n",
    "Validation corpus will be used for the intermediate validation of models. The test data will be necessary for submitting the quality of your model in the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should read *validation* corpus, located at `data/validation.tsv`. You will use it later to evaluate current solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    data = []\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        data.append(line.strip().split('\\t'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = read_corpus('data/validation.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use cosine distance to rank candidate questions which you need to implement in the function *rank_candidates*. The function should return a sorted list of pairs *(initial position in candidates list, candidate)*. Index of some pair corresponds to its rank (the first is the best). For example, if the list of candidates was *[a, b, c]* and the most similar is *c*, then *a* and *b*, the function should return a list *[(2, c), (0, a), (1, b)]*.\n",
    "\n",
    "Pay attention, if you use the function *cosine_similarity* from *sklearn.metrics.pairwise* to calculate similarity because it works in a different way: most similar objects has greatest similarity. It's preferable to use a vectorized version of *cosine_similarity* function. Try to compute similarity at once and not use list comprehension. It should speed up your computations significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates(question, candidates, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        candidates: a list of strings (candidates) which we want to rank\n",
    "        embeddings: some embeddings\n",
    "        dim: dimension of the current embeddings\n",
    "        \n",
    "        result: a list of pairs (initial position in the list, question)\n",
    "    \"\"\"\n",
    "    \n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    cos = []\n",
    "    candidates_vec = []\n",
    "    question_vec = question_to_vec(question, embeddings)\n",
    "    for i in range(len(candidates)):\n",
    "        candidates_vec.append(question_to_vec(candidates[i],embeddings).tolist())\n",
    "    cos=cosine_similarity([question_vec],candidates_vec)[0]\n",
    "    cos = np.argsort(cos)[::-1]\n",
    "    result=[]\n",
    "    for i in cos:\n",
    "        result.append((i,candidates[i]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rank_candidates():\n",
    "    questions = ['converting string to list', 'Sending array via Ajax fails']\n",
    "    candidates = [['Convert Google results object (pure js) to Python object', \n",
    "                   'C# create cookie from string and send it',\n",
    "                   'How to use jQuery AJAX for an outside domain?'], \n",
    "                  ['Getting all list items of an unordered list in PHP', \n",
    "                   'WPF- How to update the changes in list item of a list', \n",
    "                   'select2 not displaying search results']]\n",
    "    results = [[(1, 'C# create cookie from string and send it'), \n",
    "                (0, 'Convert Google results object (pure js) to Python object'), \n",
    "                (2, 'How to use jQuery AJAX for an outside domain?')],\n",
    "               [(0, 'Getting all list items of an unordered list in PHP'), \n",
    "                (2, 'select2 not displaying search results'), \n",
    "                (1, 'WPF- How to update the changes in list item of a list')]]\n",
    "    for question, q_candidates, result in zip(questions, candidates, results):\n",
    "        ranks = rank_candidates(question, q_candidates, wv_embeddings, 300)\n",
    "        if not np.all(ranks == result):\n",
    "            return \"Check the function.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the quality of the current approach. Run the next two cells to get the results. Pay attention that calculation of similarity between vectors takes time and this calculation is computed approximately in 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rank_candidates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-44ef4450b14b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrank_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mwv_ranking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mranks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rank_candidates' is not defined"
     ]
    }
   ],
   "source": [
    "wv_ranking = []\n",
    "for line in validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking, k), k, hits_count(wv_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did all the steps correctly, you should be frustrated by the received results. Let's try to understand why the quality is so low. First of all, when you work with some data it is necessary to have an idea how the data looks like. Print several questions from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to print a binary heap tree without recursion? How do you best convert a recursive function to an iterative one? How can i use ng-model with directive in angular js flash: drawing and erasing\n",
      "How to start PhoneStateListener programmatically? PhoneStateListener and service Java cast object[] to model WCF and What does this mean?\n",
      "jQuery: Show a div2 when mousenter over div1 is over when hover on div1 depenting on if it is on div2 or not it should act differently How to run selenium in google app engine/cloud? Python Comparing two lists of strings for similarities\n"
     ]
    }
   ],
   "source": [
    "for line in validation[:3]:\n",
    "    q, *examples = line\n",
    "    print(q, *examples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we deal with the raw data. It means that we have many punctuation marks, special characters and unlowercased letters. In our case, it could lead to the situation where we can't find some embeddings, e.g. for the word \"grid?\". \n",
    "\n",
    "To solve this problem you should use the functions *text_prepare* from the previous assignments to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import text_prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform all the questions from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_validation = []\n",
    "print(validation)\n",
    "for line in validation:\n",
    "    prepared_validation = text_prepare(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the approach again after the preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_prepared_ranking, k), \n",
    "                                              k, hits_count(wv_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, prepare also train and test data, because you will need it in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_file(in_, out_):\n",
    "    out = open(out_, 'w')\n",
    "    for line in open(in_, encoding='utf8'):\n",
    "        line = line.strip().split('\\t')\n",
    "        new_line = [text_prepare(q) for q in line]\n",
    "        print(*new_line, sep='\\t', file=out)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################\n",
    "prepare_file('data/train.tsv', 'data/train_prepared.tsv')\n",
    "prepare_file('data/test.tsv', 'data/test_prepared.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (W2VTokenizedRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates to earn the points. The calculations should take about 3-5 minutes. Pay attention that the function *rank_candidates* returns a ranking, while in this case you should find a position in this ranking. Ranks should start with 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import matrix_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_ranks_results = []\n",
    "prepared_test_data = 'data/test_prepared.tsv'\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings, 300)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    w2v_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('W2VTokenizedRanks', matrix_to_string(w2v_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced solution: StarSpace embeddings\n",
    "\n",
    "Now you are ready to train your own word embeddings! In particular, you need to train embeddings specially for our task of duplicates detection. Unfortunately, StarSpace cannot be run on Windows and we recommend to use provided\n",
    "[docker container](https://github.com/hse-aml/natural-language-processing/blob/master/Docker-tutorial.md) or other alternatives. Don't delete results of this task because you will need it in the final project.\n",
    "\n",
    "### How it works and what's the main difference with word2vec?\n",
    "The main point in this section is that StarSpace can be trained specifically for some tasks. In contrast to word2vec model, which tries to train similar embeddings for words in similar contexts, StarSpace uses embeddings for the whole sentence (just as a sum of embeddings of words and phrases). Despite the fact that in both cases we get word embeddings as a result of the training, StarSpace embeddings are trained using some supervised data, e.g. a set of similar sentence pairs, and thus they can better suit the task.\n",
    "\n",
    "In our case, StarSpace should use two types of sentence pairs for training: \"positive\" and \"negative\". \"Positive\" examples are extracted from the train sample (duplicates, high similarity) and the \"negative\" examples are generated randomly (low similarity assumed). \n",
    "\n",
    "### How to choose the best params for the model?\n",
    "Normally, you would start with some default choice and then run extensive experiments to compare different strategies. However, we have some recommendations ready for you to save your time:\n",
    "- Be careful with choosing the suitable training mode. In this task we want to explore texts similarity which corresponds to *trainMode = 3*.\n",
    "- Use adagrad optimization (parameter *adagrad = true*).\n",
    "- Set the length of phrase equal to 1 (parameter *ngrams*), because we need embeddings only for words.\n",
    "- Don't use a large number of *epochs* (we think that 5 should be enough).\n",
    "- Try dimension *dim* equal to 100.\n",
    "- To compare embeddings usually *cosine* *similarity* is used.\n",
    "- Set *minCount* greater than 1 (for example, 2) if you don't want to get embeddings for extremely rare words.\n",
    "- Parameter *verbose = true* could show you the progress of the training process.\n",
    "- Set parameter *fileFormat* equals *labelDoc*.\n",
    "- Parameter *negSearchLimit* is responsible for a number of negative examples which is used during the training. We think that 10 will be enought for this task.\n",
    "- To increase a speed of training we recommend to set *learning rate* to 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train StarSpace embeddings for unigrams on the train dataset. You don't need to change the format of the input data. Just don't forget to use prepared version of the training data. \n",
    "\n",
    "If you follow the instruction, the training process will take about 1 hour. The size of the embeddings' dictionary should be approximately 100 000 (number of lines in the result file). If you got significantly more than this number, try to check all the instructions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.05\n",
      "dim: 100\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: cosine\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 10\n",
      "thread: 10\n",
      "minCount: 1\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 3\n",
      "fileFormat: labelDoc\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "Start to initialize starspace model.\n",
      "Build dict from input file : data/train_prepared.tsv\n",
      "Read 12M words\n",
      "Number of words in dictionary:  205838\n",
      "Number of labels in dictionary: 0\n",
      "Loading data from file : data/train_prepared.tsv\n",
      "Total number of examples loaded : 1000000\n",
      "Initialized model weights. Model size :\n",
      "matrix : 205838 100\n",
      "Training epoch 0: 0.05 0.01\n",
      "Epoch: 65.4%  lr: 0.043811  loss: 0.010653  eta: 0h9m  tot: 0h1m21s  (13.1%)3%  lr: 0.050000  loss: 0.062885  eta: 0h12m  tot: 0h0m0s  (0.1%)0.4%  lr: 0.050000  loss: 0.058666  eta: 0h13m  tot: 0h0m0s  (0.1%)0.8%  lr: 0.049950  loss: 0.048595  eta: 0h11m  tot: 0h0m1s  (0.2%)0.9%  lr: 0.049950  loss: 0.047454  eta: 0h11m  tot: 0h0m1s  (0.2%)1.1%  lr: 0.049940  loss: 0.043175  eta: 0h11m  tot: 0h0m1s  (0.2%)1.7%  lr: 0.049920  loss: 0.038755  eta: 0h11m  tot: 0h0m2s  (0.3%)2.0%  lr: 0.049870  loss: 0.037249  eta: 0h11m  tot: 0h0m2s  (0.4%)2.1%  lr: 0.049850  loss: 0.036763  eta: 0h11m  tot: 0h0m2s  (0.4%)  lr: 0.049810  loss: 0.035002  eta: 0h11m  tot: 0h0m3s  (0.5%)3.4%  lr: 0.049730  loss: 0.030841  eta: 0h11m  tot: 0h0m4s  (0.7%)11m  tot: 0h0m5s  (0.8%)4.1%  lr: 0.049550  loss: 0.028261  eta: 0h11m  tot: 0h0m5s  (0.8%)%  lr: 0.049550  loss: 0.028234  eta: 0h11m  tot: 0h0m5s  (0.8%)4.8%  lr: 0.049510  loss: 0.026723  eta: 0h11m  tot: 0h0m6s  (1.0%)4.9%  lr: 0.049480  loss: 0.026618  eta: 0h11m  tot: 0h0m6s  (1.0%)5.4%  lr: 0.049440  loss: 0.026155  eta: 0h11m  tot: 0h0m7s  (1.1%)5.8%  lr: 0.049430  loss: 0.025647  eta: 0h11m  tot: 0h0m7s  (1.2%)5.8%  lr: 0.049430  loss: 0.025608  eta: 0h11m  tot: 0h0m7s  (1.2%)7.2%  lr: 0.049260  loss: 0.023892  eta: 0h10m  tot: 0h0m9s  (1.4%)8.5%  lr: 0.049150  loss: 0.022376  eta: 0h10m  tot: 0h0m11s  (1.7%)8.6%  lr: 0.049130  loss: 0.022266  eta: 0h10m  tot: 0h0m11s  (1.7%)9.0%  lr: 0.049080  loss: 0.021964  eta: 0h10m  tot: 0h0m11s  (1.8%)9.1%  lr: 0.049070  loss: 0.021923  eta: 0h10m  tot: 0h0m11s  (1.8%)10.3%  lr: 0.048910  loss: 0.020863  eta: 0h10m  tot: 0h0m13s  (2.1%)12.0%  lr: 0.048790  loss: 0.019745  eta: 0h10m  tot: 0h0m15s  (2.4%)12.4%  lr: 0.048770  loss: 0.019628  eta: 0h10m  tot: 0h0m15s  (2.5%)13.0%  lr: 0.048710  loss: 0.019299  eta: 0h10m  tot: 0h0m16s  (2.6%)13.5%  lr: 0.048660  loss: 0.018916  eta: 0h10m  tot: 0h0m16s  (2.7%)13.9%  lr: 0.048640  loss: 0.018733  eta: 0h10m  tot: 0h0m17s  (2.8%)13.9%  lr: 0.048640  loss: 0.018684  eta: 0h10m  tot: 0h0m17s  (2.8%)14.1%  lr: 0.048590  loss: 0.018586  eta: 0h10m  tot: 0h0m17s  (2.8%)14.8%  lr: 0.048570  loss: 0.018316  eta: 0h10m  tot: 0h0m18s  (3.0%)15.7%  lr: 0.048440  loss: 0.018069  eta: 0h10m  tot: 0h0m19s  (3.1%)16.1%  lr: 0.048400  loss: 0.017953  eta: 0h9m  tot: 0h0m19s  (3.2%)16.5%  lr: 0.048320  loss: 0.017788  eta: 0h9m  tot: 0h0m20s  (3.3%)17.0%  lr: 0.048260  loss: 0.017573  eta: 0h9m  tot: 0h0m21s  (3.4%)17.4%  lr: 0.048250  loss: 0.017403  eta: 0h9m  tot: 0h0m21s  (3.5%)17.8%  lr: 0.048220  loss: 0.017266  eta: 0h9m  tot: 0h0m22s  (3.6%)18.1%  lr: 0.048160  loss: 0.017182  eta: 0h9m  tot: 0h0m22s  (3.6%)19.0%  lr: 0.048080  loss: 0.016964  eta: 0h9m  tot: 0h0m23s  (3.8%)19.4%  lr: 0.048060  loss: 0.016769  eta: 0h9m  tot: 0h0m24s  (3.9%)19.8%  lr: 0.048050  loss: 0.016613  eta: 0h9m  tot: 0h0m24s  (4.0%)20.5%  lr: 0.047980  loss: 0.016402  eta: 0h9m  tot: 0h0m25s  (4.1%)21.2%  lr: 0.047910  loss: 0.016151  eta: 0h9m  tot: 0h0m26s  (4.2%)22.0%  lr: 0.047880  loss: 0.015954  eta: 0h9m  tot: 0h0m26s  (4.4%)22.9%  lr: 0.047740  loss: 0.015701  eta: 0h9m  tot: 0h0m27s  (4.6%)%  lr: 0.047720  loss: 0.015598  eta: 0h9m  tot: 0h0m28s  (4.7%)24.5%  lr: 0.047650  loss: 0.015389  eta: 0h9m  tot: 0h0m29s  (4.9%)25.1%  lr: 0.047590  loss: 0.015236  eta: 0h9m  tot: 0h0m30s  (5.0%)25.4%  lr: 0.047580  loss: 0.015163  eta: 0h9m  tot: 0h0m30s  (5.1%)26.1%  lr: 0.047490  loss: 0.015042  eta: 0h9m  tot: 0h0m31s  (5.2%)26.7%  lr: 0.047470  loss: 0.014931  eta: 0h9m  tot: 0h0m31s  (5.3%)27.6%  lr: 0.047410  loss: 0.014785  eta: 0h9m  tot: 0h0m32s  (5.5%)28.2%  lr: 0.047350  loss: 0.014710  eta: 0h9m  tot: 0h0m33s  (5.6%)28.3%  lr: 0.047350  loss: 0.014704  eta: 0h9m  tot: 0h0m33s  (5.7%)29.2%  lr: 0.047250  loss: 0.014506  eta: 0h9m  tot: 0h0m35s  (5.8%)29.9%  lr: 0.047180  loss: 0.014386  eta: 0h9m  tot: 0h0m35s  (6.0%)30.4%  lr: 0.047140  loss: 0.014293  eta: 0h9m  tot: 0h0m36s  (6.1%)30.8%  lr: 0.047110  loss: 0.014225  eta: 0h9m  tot: 0h0m37s  (6.2%)31.0%  lr: 0.047110  loss: 0.014170  eta: 0h9m  tot: 0h0m37s  (6.2%)32.2%  lr: 0.047030  loss: 0.014032  eta: 0h9m  tot: 0h0m39s  (6.4%)32.5%  lr: 0.047020  loss: 0.013980  eta: 0h9m  tot: 0h0m39s  (6.5%)32.6%  lr: 0.047010  loss: 0.013988  eta: 0h9m  tot: 0h0m39s  (6.5%)33.5%  lr: 0.046970  loss: 0.013847  eta: 0h9m  tot: 0h0m40s  (6.7%)34.4%  lr: 0.046890  loss: 0.013717  eta: 0h9m  tot: 0h0m41s  (6.9%)%  lr: 0.046870  loss: 0.013637  eta: 0h9m  tot: 0h0m42s  (7.0%)35.2%  lr: 0.046800  loss: 0.013567  eta: 0h9m  tot: 0h0m42s  (7.0%)35.3%  lr: 0.046780  loss: 0.013550  eta: 0h9m  tot: 0h0m42s  (7.1%)35.4%  lr: 0.046780  loss: 0.013533  eta: 0h9m  tot: 0h0m42s  (7.1%)35.5%  lr: 0.046760  loss: 0.013491  eta: 0h9m  tot: 0h0m43s  (7.1%)36.6%  lr: 0.046650  loss: 0.013316  eta: 0h9m  tot: 0h0m44s  (7.3%)37.0%  lr: 0.046620  loss: 0.013265  eta: 0h9m  tot: 0h0m44s  (7.4%)37.1%  lr: 0.046620  loss: 0.013261  eta: 0h9m  tot: 0h0m44s  (7.4%)37.3%  lr: 0.046600  loss: 0.013234  eta: 0h9m  tot: 0h0m45s  (7.5%)37.6%  lr: 0.046580  loss: 0.013183  eta: 0h9m  tot: 0h0m45s  (7.5%)38.4%  lr: 0.046490  loss: 0.013049  eta: 0h9m  tot: 0h0m46s  (7.7%)39.1%  lr: 0.046420  loss: 0.012932  eta: 0h9m  tot: 0h0m47s  (7.8%)39.3%  lr: 0.046420  loss: 0.012915  eta: 0h9m  tot: 0h0m47s  (7.9%)39.5%  lr: 0.046370  loss: 0.012894  eta: 0h9m  tot: 0h0m47s  (7.9%)39.9%  lr: 0.046290  loss: 0.012850  eta: 0h9m  tot: 0h0m48s  (8.0%)40.0%  lr: 0.046260  loss: 0.012838  eta: 0h9m  tot: 0h0m48s  (8.0%)40.7%  lr: 0.046211  loss: 0.012776  eta: 0h9m  tot: 0h0m49s  (8.1%)41.7%  lr: 0.046151  loss: 0.012629  eta: 0h9m  tot: 0h0m50s  (8.3%)  lr: 0.046141  loss: 0.012605  eta: 0h9m  tot: 0h0m50s  (8.4%)41.9%  lr: 0.046131  loss: 0.012595  eta: 0h9m  tot: 0h0m50s  (8.4%)42.4%  lr: 0.046081  loss: 0.012523  eta: 0h9m  tot: 0h0m51s  (8.5%)42.7%  lr: 0.046081  loss: 0.012487  eta: 0h9m  tot: 0h0m51s  (8.5%)42.9%  lr: 0.046041  loss: 0.012464  eta: 0h9m  tot: 0h0m51s  (8.6%)44.1%  lr: 0.045931  loss: 0.012298  eta: 0h9m  tot: 0h0m53s  (8.8%)45.9%  lr: 0.045771  loss: 0.012125  eta: 0h9m  tot: 0h0m55s  (9.2%)%  lr: 0.045541  loss: 0.011939  eta: 0h9m  tot: 0h0m57s  (9.5%)48.5%  lr: 0.045501  loss: 0.011845  eta: 0h9m  tot: 0h0m59s  (9.7%)48.9%  lr: 0.045471  loss: 0.011839  eta: 0h9m  tot: 0h0m59s  (9.8%)49.7%  lr: 0.045421  loss: 0.011741  eta: 0h9m  tot: 0h1m0s  (9.9%)49.9%  lr: 0.045401  loss: 0.011706  eta: 0h9m  tot: 0h1m1s  (10.0%)50.0%  lr: 0.045391  loss: 0.011699  eta: 0h9m  tot: 0h1m1s  (10.0%)50.4%  lr: 0.045371  loss: 0.011676  eta: 0h9m  tot: 0h1m2s  (10.1%)h9m  tot: 0h1m2s  (10.1%)  loss: 0.011627  eta: 0h9m  tot: 0h1m3s  (10.2%)51.9%  lr: 0.045151  loss: 0.011565  eta: 0h9m  tot: 0h1m4s  (10.4%)52.3%  lr: 0.045081  loss: 0.011534  eta: 0h9m  tot: 0h1m5s  (10.5%)54.4%  lr: 0.044921  loss: 0.011365  eta: 0h9m  tot: 0h1m8s  (10.9%)54.5%  lr: 0.044921  loss: 0.011371  eta: 0h9m  tot: 0h1m8s  (10.9%)54.8%  lr: 0.044921  loss: 0.011350  eta: 0h9m  tot: 0h1m8s  (11.0%)55.0%  lr: 0.044881  loss: 0.011335  eta: 0h9m  tot: 0h1m8s  (11.0%)55.6%  lr: 0.044821  loss: 0.011293  eta: 0h9m  tot: 0h1m9s  (11.1%)56.5%  lr: 0.044711  loss: 0.011218  eta: 0h9m  tot: 0h1m10s  (11.3%)57.4%  lr: 0.044651  loss: 0.011157  eta: 0h9m  tot: 0h1m11s  (11.5%)58.2%  lr: 0.044571  loss: 0.011105  eta: 0h9m  tot: 0h1m12s  (11.6%)59.4%  lr: 0.044471  loss: 0.011022  eta: 0h9m  tot: 0h1m13s  (11.9%)59.6%  lr: 0.044431  loss: 0.011010  eta: 0h9m  tot: 0h1m14s  (11.9%)60.2%  lr: 0.044381  loss: 0.010979  eta: 0h9m  tot: 0h1m14s  (12.0%)60.7%  lr: 0.044311  loss: 0.010971  eta: 0h9m  tot: 0h1m15s  (12.1%)%  lr: 0.044301  loss: 0.010960  eta: 0h9m  tot: 0h1m15s  (12.2%)61.0%  lr: 0.044301  loss: 0.010960  eta: 0h9m  tot: 0h1m15s  (12.2%)61.5%  lr: 0.044241  loss: 0.010930  eta: 0h9m  tot: 0h1m16s  (12.3%)62.2%  lr: 0.044151  loss: 0.010874  eta: 0h9m  tot: 0h1m17s  (12.4%)62.8%  lr: 0.044081  loss: 0.010837  eta: 0h9m  tot: 0h1m18s  (12.6%)64.3%  lr: 0.043941  loss: 0.010740  eta: 0h9m  tot: 0h1m20s  (12.9%)64.3%  lr: 0.043921  loss: 0.010735  eta: 0h9m  tot: 0h1m20s  (12.9%)65.0%  lr: 0.043881  loss: 0.010687  eta: 0h9m  tot: 0h1m21s  (13.0%)65.5%  lr: 0.043791  loss: 0.010651  eta: 0h9m  tot: 0h1m21s  (13.1%)Epoch: 100.0%  lr: 0.040301  loss: 0.009148  eta: 0h8m  tot: 0h2m0s  (20.0%)67.0%  lr: 0.043611  loss: 0.010573  eta: 0h8m  tot: 0h1m23s  (13.4%)67.6%  lr: 0.043541  loss: 0.010538  eta: 0h8m  tot: 0h1m24s  (13.5%)67.8%  lr: 0.043521  loss: 0.010527  eta: 0h8m  tot: 0h1m24s  (13.6%)68.1%  lr: 0.043511  loss: 0.010503  eta: 0h8m  tot: 0h1m24s  (13.6%)68.6%  lr: 0.043451  loss: 0.010487  eta: 0h8m  tot: 0h1m25s  (13.7%)69.0%  lr: 0.043391  loss: 0.010462  eta: 0h8m  tot: 0h1m25s  (13.8%)69.2%  lr: 0.043371  loss: 0.010448  eta: 0h8m  tot: 0h1m25s  (13.8%)69.4%  lr: 0.043351  loss: 0.010444  eta: 0h8m  tot: 0h1m26s  (13.9%)69.9%  lr: 0.043331  loss: 0.010417  eta: 0h8m  tot: 0h1m26s  (14.0%)71.4%  lr: 0.043171  loss: 0.010333  eta: 0h8m  tot: 0h1m28s  (14.3%)71.6%  lr: 0.043121  loss: 0.010318  eta: 0h8m  tot: 0h1m28s  (14.3%)71.8%  lr: 0.043121  loss: 0.010304  eta: 0h8m  tot: 0h1m29s  (14.4%)72.0%  lr: 0.043121  loss: 0.010288  eta: 0h8m  tot: 0h1m29s  (14.4%)72.3%  lr: 0.043111  loss: 0.010274  eta: 0h8m  tot: 0h1m29s  (14.5%)72.9%  lr: 0.043041  loss: 0.010234  eta: 0h8m  tot: 0h1m30s  (14.6%)73.3%  lr: 0.042981  loss: 0.010213  eta: 0h8m  tot: 0h1m30s  (14.7%)74.2%  lr: 0.042871  loss: 0.010176  eta: 0h8m  tot: 0h1m31s  (14.8%)74.3%  lr: 0.042871  loss: 0.010166  eta: 0h8m  tot: 0h1m32s  (14.9%)74.5%  lr: 0.042861  loss: 0.010156  eta: 0h8m  tot: 0h1m32s  (14.9%)75.2%  lr: 0.042821  loss: 0.010135  eta: 0h8m  tot: 0h1m33s  (15.0%)76.2%  lr: 0.042741  loss: 0.010094  eta: 0h8m  tot: 0h1m34s  (15.2%)76.8%  lr: 0.042691  loss: 0.010068  eta: 0h8m  tot: 0h1m34s  (15.4%)77.2%  lr: 0.042661  loss: 0.010047  eta: 0h8m  tot: 0h1m35s  (15.4%)77.7%  lr: 0.042631  loss: 0.010023  eta: 0h8m  tot: 0h1m35s  (15.5%)77.8%  lr: 0.042621  loss: 0.010014  eta: 0h8m  tot: 0h1m36s  (15.6%)78.0%  lr: 0.042591  loss: 0.010004  eta: 0h8m  tot: 0h1m36s  (15.6%)79.1%  lr: 0.042521  loss: 0.009968  eta: 0h8m  tot: 0h1m37s  (15.8%)79.3%  lr: 0.042511  loss: 0.009958  eta: 0h8m  tot: 0h1m37s  (15.9%)79.4%  lr: 0.042511  loss: 0.009952  eta: 0h8m  tot: 0h1m38s  (15.9%)79.6%  lr: 0.042501  loss: 0.009945  eta: 0h8m  tot: 0h1m38s  (15.9%)79.7%  lr: 0.042481  loss: 0.009945  eta: 0h8m  tot: 0h1m38s  (15.9%)80.1%  lr: 0.042451  loss: 0.009930  eta: 0h8m  tot: 0h1m38s  (16.0%)80.4%  lr: 0.042431  loss: 0.009915  eta: 0h8m  tot: 0h1m39s  (16.1%)80.6%  lr: 0.042401  loss: 0.009903  eta: 0h8m  tot: 0h1m39s  (16.1%)80.9%  lr: 0.042391  loss: 0.009893  eta: 0h8m  tot: 0h1m39s  (16.2%)81.4%  lr: 0.042301  loss: 0.009874  eta: 0h8m  tot: 0h1m40s  (16.3%)82.0%  lr: 0.042251  loss: 0.009854  eta: 0h8m  tot: 0h1m41s  (16.4%)83.4%  lr: 0.042111  loss: 0.009785  eta: 0h8m  tot: 0h1m42s  (16.7%)83.7%  lr: 0.042091  loss: 0.009770  eta: 0h8m  tot: 0h1m42s  (16.7%)84.0%  lr: 0.042071  loss: 0.009754  eta: 0h8m  tot: 0h1m43s  (16.8%)84.2%  lr: 0.042051  loss: 0.009735  eta: 0h8m  tot: 0h1m43s  (16.8%)84.8%  lr: 0.042001  loss: 0.009721  eta: 0h8m  tot: 0h1m44s  (17.0%)85.3%  lr: 0.041941  loss: 0.009698  eta: 0h8m  tot: 0h1m44s  (17.1%)85.7%  lr: 0.041841  loss: 0.009687  eta: 0h8m  tot: 0h1m45s  (17.1%)87.0%  lr: 0.041671  loss: 0.009630  eta: 0h8m  tot: 0h1m46s  (17.4%)%  lr: 0.041601  loss: 0.009612  eta: 0h8m  tot: 0h1m47s  (17.5%)87.7%  lr: 0.041571  loss: 0.009613  eta: 0h8m  tot: 0h1m47s  (17.5%)88.1%  lr: 0.041551  loss: 0.009594  eta: 0h8m  tot: 0h1m48s  (17.6%)88.6%  lr: 0.041481  loss: 0.009566  eta: 0h8m  tot: 0h1m48s  (17.7%)90.1%  lr: 0.041311  loss: 0.009509  eta: 0h8m  tot: 0h1m50s  (18.0%)  lr: 0.041231  loss: 0.009493  eta: 0h8m  tot: 0h1m50s  (18.1%)91.0%  lr: 0.041191  loss: 0.009470  eta: 0h8m  tot: 0h1m51s  (18.2%)91.2%  lr: 0.041161  loss: 0.009465  eta: 0h8m  tot: 0h1m51s  (18.2%)91.3%  lr: 0.041151  loss: 0.009456  eta: 0h8m  tot: 0h1m51s  (18.3%)91.8%  lr: 0.041131  loss: 0.009439  eta: 0h8m  tot: 0h1m52s  (18.4%)92.1%  lr: 0.041111  loss: 0.009427  eta: 0h8m  tot: 0h1m52s  (18.4%)93.3%  lr: 0.040921  loss: 0.009366  eta: 0h8m  tot: 0h1m53s  (18.7%)%  lr: 0.040911  loss: 0.009362  eta: 0h8m  tot: 0h1m53s  (18.7%)93.6%  lr: 0.040901  loss: 0.009364  eta: 0h8m  tot: 0h1m53s  (18.7%)94.3%  lr: 0.040831  loss: 0.009340  eta: 0h8m  tot: 0h1m54s  (18.9%)94.7%  lr: 0.040801  loss: 0.009320  eta: 0h8m  tot: 0h1m55s  (18.9%)94.9%  lr: 0.040781  loss: 0.009312  eta: 0h8m  tot: 0h1m55s  (19.0%)95.8%  lr: 0.040691  loss: 0.009275  eta: 0h8m  tot: 0h1m56s  (19.2%)95.9%  lr: 0.040671  loss: 0.009273  eta: 0h8m  tot: 0h1m56s  (19.2%)96.7%  lr: 0.040651  loss: 0.009254  eta: 0h8m  tot: 0h1m57s  (19.3%)96.9%  lr: 0.040631  loss: 0.009242  eta: 0h8m  tot: 0h1m57s  (19.4%)97.6%  lr: 0.040481  loss: 0.009232  eta: 0h8m  tot: 0h1m58s  (19.5%)98.0%  lr: 0.040441  loss: 0.009216  eta: 0h8m  tot: 0h1m58s  (19.6%)98.2%  lr: 0.040391  loss: 0.009213  eta: 0h8m  tot: 0h1m58s  (19.6%)98.6%  lr: 0.040371  loss: 0.009199  eta: 0h8m  tot: 0h1m59s  (19.7%)99.8%  lr: 0.040321  loss: 0.009159  eta: 0h8m  tot: 0h2m0s  (20.0%)\n",
      " ---+++                Epoch    0 Train error : 0.00904862 +++--- ���\n",
      "Training epoch 1: 0.04 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71.0%  lr: 0.032911  loss: 0.002419  eta: 0h6m  tot: 0h3m34s  (34.2%)6%  lr: 0.039920  loss: 0.002888  eta: 0h6m  tot: 0h2m6s  (20.1%)1.2%  lr: 0.039880  loss: 0.002378  eta: 0h6m  tot: 0h2m7s  (20.2%)3.4%  lr: 0.039620  loss: 0.002474  eta: 0h8m  tot: 0h2m10s  (20.7%)4.5%  lr: 0.039500  loss: 0.002497  eta: 0h8m  tot: 0h2m11s  (20.9%)  lr: 0.039480  loss: 0.002472  eta: 0h8m  tot: 0h2m12s  (20.9%)5.1%  lr: 0.039420  loss: 0.002497  eta: 0h8m  tot: 0h2m12s  (21.0%)  eta: 0h8m  tot: 0h2m13s  (21.1%)5.8%  lr: 0.039310  loss: 0.002484  eta: 0h8m  tot: 0h2m13s  (21.2%)6.2%  lr: 0.039240  loss: 0.002472  eta: 0h8m  tot: 0h2m14s  (21.2%)7.1%  lr: 0.039170  loss: 0.002438  eta: 0h8m  tot: 0h2m15s  (21.4%)7.8%  lr: 0.039130  loss: 0.002447  eta: 0h8m  tot: 0h2m15s  (21.6%)8.8%  lr: 0.039020  loss: 0.002408  eta: 0h8m  tot: 0h2m17s  (21.8%)9.1%  lr: 0.039010  loss: 0.002397  eta: 0h8m  tot: 0h2m17s  (21.8%)9.2%  lr: 0.039000  loss: 0.002394  eta: 0h8m  tot: 0h2m17s  (21.8%)10.4%  lr: 0.038910  loss: 0.002400  eta: 0h8m  tot: 0h2m18s  (22.1%)11.2%  lr: 0.038820  loss: 0.002369  eta: 0h7m  tot: 0h2m19s  (22.2%)11.4%  lr: 0.038790  loss: 0.002353  eta: 0h7m  tot: 0h2m19s  (22.3%)11.5%  lr: 0.038790  loss: 0.002356  eta: 0h7m  tot: 0h2m19s  (22.3%)12.0%  lr: 0.038750  loss: 0.002346  eta: 0h7m  tot: 0h2m20s  (22.4%)12.1%  lr: 0.038750  loss: 0.002347  eta: 0h7m  tot: 0h2m20s  (22.4%)12.7%  lr: 0.038680  loss: 0.002332  eta: 0h8m  tot: 0h2m21s  (22.5%)13.1%  lr: 0.038670  loss: 0.002349  eta: 0h7m  tot: 0h2m22s  (22.6%)13.8%  lr: 0.038630  loss: 0.002373  eta: 0h7m  tot: 0h2m22s  (22.8%)14.0%  lr: 0.038570  loss: 0.002392  eta: 0h7m  tot: 0h2m23s  (22.8%)14.3%  lr: 0.038530  loss: 0.002419  eta: 0h7m  tot: 0h2m23s  (22.9%)15.5%  lr: 0.038460  loss: 0.002404  eta: 0h7m  tot: 0h2m24s  (23.1%)15.7%  lr: 0.038420  loss: 0.002405  eta: 0h7m  tot: 0h2m25s  (23.1%)15.9%  lr: 0.038410  loss: 0.002404  eta: 0h7m  tot: 0h2m25s  (23.2%)16.5%  lr: 0.038370  loss: 0.002429  eta: 0h7m  tot: 0h2m26s  (23.3%)16.7%  lr: 0.038340  loss: 0.002443  eta: 0h7m  tot: 0h2m26s  (23.3%)17.2%  lr: 0.038300  loss: 0.002427  eta: 0h7m  tot: 0h2m26s  (23.4%)17.5%  lr: 0.038280  loss: 0.002423  eta: 0h7m  tot: 0h2m27s  (23.5%)17.6%  lr: 0.038280  loss: 0.002424  eta: 0h7m  tot: 0h2m27s  (23.5%)18.4%  lr: 0.038240  loss: 0.002404  eta: 0h7m  tot: 0h2m28s  (23.7%)18.4%  lr: 0.038240  loss: 0.002410  eta: 0h7m  tot: 0h2m28s  (23.7%)19.8%  lr: 0.038150  loss: 0.002406  eta: 0h7m  tot: 0h2m29s  (24.0%)19.9%  lr: 0.038140  loss: 0.002403  eta: 0h7m  tot: 0h2m29s  (24.0%)21.2%  lr: 0.037980  loss: 0.002389  eta: 0h7m  tot: 0h2m31s  (24.2%)21.4%  lr: 0.037960  loss: 0.002393  eta: 0h7m  tot: 0h2m32s  (24.3%)22.0%  lr: 0.037870  loss: 0.002395  eta: 0h7m  tot: 0h2m32s  (24.4%)22.7%  lr: 0.037820  loss: 0.002406  eta: 0h7m  tot: 0h2m33s  (24.5%)26.6%  lr: 0.037460  loss: 0.002416  eta: 0h7m  tot: 0h2m38s  (25.3%)26.7%  lr: 0.037450  loss: 0.002418  eta: 0h7m  tot: 0h2m38s  (25.3%)%  lr: 0.037420  loss: 0.002417  eta: 0h7m  tot: 0h2m39s  (25.5%)27.7%  lr: 0.037370  loss: 0.002432  eta: 0h7m  tot: 0h2m39s  (25.5%)29.0%  lr: 0.037280  loss: 0.002422  eta: 0h7m  tot: 0h2m41s  (25.8%)29.5%  lr: 0.037200  loss: 0.002418  eta: 0h7m  tot: 0h2m42s  (25.9%)30.0%  lr: 0.037140  loss: 0.002413  eta: 0h7m  tot: 0h2m42s  (26.0%)30.8%  lr: 0.037070  loss: 0.002416  eta: 0h7m  tot: 0h2m43s  (26.2%)32.9%  lr: 0.036850  loss: 0.002389  eta: 0h7m  tot: 0h2m46s  (26.6%)33.7%  lr: 0.036770  loss: 0.002382  eta: 0h7m  tot: 0h2m47s  (26.7%)%  lr: 0.036760  loss: 0.002377  eta: 0h7m  tot: 0h2m47s  (26.8%)35.3%  lr: 0.036640  loss: 0.002374  eta: 0h7m  tot: 0h2m48s  (27.1%)36.0%  lr: 0.036610  loss: 0.002372  eta: 0h7m  tot: 0h2m49s  (27.2%)36.5%  lr: 0.036570  loss: 0.002370  eta: 0h7m  tot: 0h2m50s  (27.3%)36.7%  lr: 0.036570  loss: 0.002367  eta: 0h7m  tot: 0h2m50s  (27.3%)38.3%  lr: 0.036400  loss: 0.002387  eta: 0h7m  tot: 0h2m52s  (27.7%)38.4%  lr: 0.036400  loss: 0.002386  eta: 0h7m  tot: 0h2m52s  (27.7%)39.8%  lr: 0.036260  loss: 0.002395  eta: 0h7m  tot: 0h2m54s  (28.0%)40.0%  lr: 0.036230  loss: 0.002392  eta: 0h7m  tot: 0h2m54s  (28.0%)40.4%  lr: 0.036181  loss: 0.002382  eta: 0h7m  tot: 0h2m55s  (28.1%)40.5%  lr: 0.036171  loss: 0.002380  eta: 0h7m  tot: 0h2m55s  (28.1%)40.6%  lr: 0.036141  loss: 0.002391  eta: 0h7m  tot: 0h2m55s  (28.1%)40.9%  lr: 0.036141  loss: 0.002387  eta: 0h7m  tot: 0h2m55s  (28.2%)42.1%  lr: 0.035981  loss: 0.002381  eta: 0h7m  tot: 0h2m57s  (28.4%)43.7%  lr: 0.035771  loss: 0.002412  eta: 0h7m  tot: 0h2m59s  (28.7%)43.9%  lr: 0.035741  loss: 0.002411  eta: 0h7m  tot: 0h2m59s  (28.8%)44.5%  lr: 0.035741  loss: 0.002414  eta: 0h7m  tot: 0h3m0s  (28.9%)44.7%  lr: 0.035711  loss: 0.002412  eta: 0h7m  tot: 0h3m0s  (28.9%)44.8%  lr: 0.035711  loss: 0.002413  eta: 0h7m  tot: 0h3m0s  (29.0%)45.1%  lr: 0.035661  loss: 0.002417  eta: 0h7m  tot: 0h3m1s  (29.0%)46.5%  lr: 0.035541  loss: 0.002416  eta: 0h7m  tot: 0h3m2s  (29.3%)46.6%  lr: 0.035541  loss: 0.002415  eta: 0h7m  tot: 0h3m2s  (29.3%)47.2%  lr: 0.035461  loss: 0.002419  eta: 0h7m  tot: 0h3m3s  (29.4%)47.5%  lr: 0.035461  loss: 0.002422  eta: 0h7m  tot: 0h3m4s  (29.5%)48.2%  lr: 0.035451  loss: 0.002422  eta: 0h7m  tot: 0h3m4s  (29.6%)49.8%  lr: 0.035271  loss: 0.002422  eta: 0h7m  tot: 0h3m6s  (30.0%)50.0%  lr: 0.035251  loss: 0.002417  eta: 0h7m  tot: 0h3m6s  (30.0%)50.2%  lr: 0.035251  loss: 0.002421  eta: 0h7m  tot: 0h3m7s  (30.0%)50.4%  lr: 0.035201  loss: 0.002420  eta: 0h7m  tot: 0h3m7s  (30.1%)50.6%  lr: 0.035171  loss: 0.002421  eta: 0h7m  tot: 0h3m7s  (30.1%)51.3%  lr: 0.035101  loss: 0.002432  eta: 0h7m  tot: 0h3m8s  (30.3%)51.4%  lr: 0.035091  loss: 0.002429  eta: 0h7m  tot: 0h3m8s  (30.3%)51.7%  lr: 0.035041  loss: 0.002429  eta: 0h7m  tot: 0h3m8s  (30.3%)52.4%  lr: 0.034971  loss: 0.002433  eta: 0h7m  tot: 0h3m9s  (30.5%)7m  tot: 0h3m9s  (30.5%)%  lr: 0.034921  loss: 0.002445  eta: 0h7m  tot: 0h3m10s  (30.6%)53.6%  lr: 0.034881  loss: 0.002453  eta: 0h7m  tot: 0h3m11s  (30.7%)53.7%  lr: 0.034881  loss: 0.002453  eta: 0h7m  tot: 0h3m11s  (30.7%)54.2%  lr: 0.034811  loss: 0.002457  eta: 0h7m  tot: 0h3m11s  (30.8%)54.4%  lr: 0.034771  loss: 0.002452  eta: 0h7m  tot: 0h3m12s  (30.9%)54.6%  lr: 0.034761  loss: 0.002455  eta: 0h7m  tot: 0h3m12s  (30.9%)54.7%  lr: 0.034751  loss: 0.002453  eta: 0h7m  tot: 0h3m12s  (30.9%)55.1%  lr: 0.034721  loss: 0.002454  eta: 0h7m  tot: 0h3m13s  (31.0%)55.8%  lr: 0.034641  loss: 0.002449  eta: 0h6m  tot: 0h3m13s  (31.2%)56.1%  lr: 0.034631  loss: 0.002450  eta: 0h6m  tot: 0h3m14s  (31.2%)0h3m14s  (31.2%)  lr: 0.034611  loss: 0.002446  eta: 0h6m  tot: 0h3m14s  (31.2%)56.9%  lr: 0.034541  loss: 0.002445  eta: 0h6m  tot: 0h3m15s  (31.4%)57.4%  lr: 0.034471  loss: 0.002436  eta: 0h6m  tot: 0h3m15s  (31.5%)58.1%  lr: 0.034371  loss: 0.002436  eta: 0h6m  tot: 0h3m16s  (31.6%)%  lr: 0.034291  loss: 0.002437  eta: 0h6m  tot: 0h3m17s  (31.8%)60.2%  lr: 0.034121  loss: 0.002432  eta: 0h6m  tot: 0h3m19s  (32.0%)60.3%  lr: 0.034121  loss: 0.002430  eta: 0h6m  tot: 0h3m19s  (32.1%)60.6%  lr: 0.034101  loss: 0.002427  eta: 0h6m  tot: 0h3m20s  (32.1%)60.8%  lr: 0.034051  loss: 0.002425  eta: 0h6m  tot: 0h3m20s  (32.2%)61.6%  lr: 0.033951  loss: 0.002424  eta: 0h6m  tot: 0h3m21s  (32.3%)61.8%  lr: 0.033921  loss: 0.002424  eta: 0h6m  tot: 0h3m22s  (32.4%)63.7%  lr: 0.033721  loss: 0.002416  eta: 0h6m  tot: 0h3m24s  (32.7%)64.3%  lr: 0.033681  loss: 0.002413  eta: 0h6m  tot: 0h3m25s  (32.9%)64.7%  lr: 0.033641  loss: 0.002418  eta: 0h6m  tot: 0h3m25s  (32.9%)  tot: 0h3m26s  (33.0%)%  lr: 0.033601  loss: 0.002421  eta: 0h6m  tot: 0h3m26s  (33.0%)6m  tot: 0h3m26s  (33.0%)66.7%  lr: 0.033411  loss: 0.002424  eta: 0h6m  tot: 0h3m28s  (33.3%)68.4%  lr: 0.033201  loss: 0.002420  eta: 0h6m  tot: 0h3m31s  (33.7%)68.5%  lr: 0.033201  loss: 0.002425  eta: 0h6m  tot: 0h3m31s  (33.7%)68.8%  lr: 0.033181  loss: 0.002426  eta: 0h6m  tot: 0h3m32s  (33.8%)70.6%  lr: 0.032951  loss: 0.002419  eta: 0h6m  tot: 0h3m34s  (34.1%)70.7%  lr: 0.032951  loss: 0.002420  eta: 0h6m  tot: 0h3m34s  (34.1%)70.9%  lr: 0.032941  loss: 0.002420  eta: 0h6m  tot: 0h3m34s  (34.2%)71.1%  lr: 0.032891  loss: 0.002418  eta: 0h6m  tot: 0h3m34s  (34.2%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.030151  loss: 0.002409  eta: 0h6m  tot: 0h4m8s  (40.0%)71.6%  lr: 0.032861  loss: 0.002419  eta: 0h6m  tot: 0h3m35s  (34.3%)72.2%  lr: 0.032831  loss: 0.002418  eta: 0h6m  tot: 0h3m36s  (34.4%)72.3%  lr: 0.032811  loss: 0.002419  eta: 0h6m  tot: 0h3m36s  (34.5%)72.5%  lr: 0.032811  loss: 0.002419  eta: 0h6m  tot: 0h3m36s  (34.5%)72.8%  lr: 0.032781  loss: 0.002420  eta: 0h6m  tot: 0h3m37s  (34.6%)74.2%  lr: 0.032691  loss: 0.002415  eta: 0h6m  tot: 0h3m38s  (34.8%)74.5%  lr: 0.032651  loss: 0.002417  eta: 0h6m  tot: 0h3m38s  (34.9%)74.6%  lr: 0.032651  loss: 0.002416  eta: 0h6m  tot: 0h3m39s  (34.9%)75.3%  lr: 0.032601  loss: 0.002422  eta: 0h6m  tot: 0h3m39s  (35.1%)75.7%  lr: 0.032571  loss: 0.002420  eta: 0h6m  tot: 0h3m40s  (35.1%)76.2%  lr: 0.032511  loss: 0.002423  eta: 0h6m  tot: 0h3m40s  (35.2%)76.3%  lr: 0.032511  loss: 0.002423  eta: 0h6m  tot: 0h3m41s  (35.3%)76.4%  lr: 0.032501  loss: 0.002424  eta: 0h6m  tot: 0h3m41s  (35.3%)76.7%  lr: 0.032481  loss: 0.002422  eta: 0h6m  tot: 0h3m41s  (35.3%)77.3%  lr: 0.032351  loss: 0.002423  eta: 0h6m  tot: 0h3m42s  (35.5%)77.7%  lr: 0.032321  loss: 0.002419  eta: 0h6m  tot: 0h3m42s  (35.5%)77.8%  lr: 0.032301  loss: 0.002418  eta: 0h6m  tot: 0h3m42s  (35.6%)78.7%  lr: 0.032171  loss: 0.002418  eta: 0h6m  tot: 0h3m43s  (35.7%)79.3%  lr: 0.032141  loss: 0.002414  eta: 0h6m  tot: 0h3m44s  (35.9%)%  lr: 0.032131  loss: 0.002414  eta: 0h6m  tot: 0h3m44s  (35.9%)79.9%  lr: 0.032091  loss: 0.002415  eta: 0h6m  tot: 0h3m45s  (36.0%)80.0%  lr: 0.032081  loss: 0.002414  eta: 0h6m  tot: 0h3m45s  (36.0%)81.8%  lr: 0.031951  loss: 0.002420  eta: 0h6m  tot: 0h3m47s  (36.4%)82.7%  lr: 0.031901  loss: 0.002414  eta: 0h6m  tot: 0h3m48s  (36.5%)%  lr: 0.031871  loss: 0.002415  eta: 0h6m  tot: 0h3m48s  (36.6%)83.5%  lr: 0.031811  loss: 0.002409  eta: 0h6m  tot: 0h3m49s  (36.7%)%  lr: 0.031771  loss: 0.002408  eta: 0h6m  tot: 0h3m49s  (36.8%)84.0%  lr: 0.031741  loss: 0.002411  eta: 0h6m  tot: 0h3m49s  (36.8%)84.3%  lr: 0.031721  loss: 0.002410  eta: 0h6m  tot: 0h3m50s  (36.9%)0.002406  eta: 0h6m  tot: 0h3m54s  (37.4%)87.4%  lr: 0.031351  loss: 0.002406  eta: 0h6m  tot: 0h3m54s  (37.5%)87.6%  lr: 0.031351  loss: 0.002407  eta: 0h6m  tot: 0h3m54s  (37.5%)87.8%  lr: 0.031341  loss: 0.002409  eta: 0h6m  tot: 0h3m55s  (37.6%)89.3%  lr: 0.031151  loss: 0.002413  eta: 0h6m  tot: 0h3m56s  (37.9%)89.5%  lr: 0.031141  loss: 0.002412  eta: 0h6m  tot: 0h3m56s  (37.9%)90.0%  lr: 0.031071  loss: 0.002413  eta: 0h6m  tot: 0h3m57s  (38.0%)90.4%  lr: 0.031051  loss: 0.002409  eta: 0h6m  tot: 0h3m57s  (38.1%)90.6%  lr: 0.031041  loss: 0.002407  eta: 0h6m  tot: 0h3m58s  (38.1%)90.9%  lr: 0.031011  loss: 0.002406  eta: 0h6m  tot: 0h3m58s  (38.2%)91.1%  lr: 0.030981  loss: 0.002404  eta: 0h6m  tot: 0h3m58s  (38.2%)92.0%  lr: 0.030881  loss: 0.002404  eta: 0h6m  tot: 0h3m59s  (38.4%)92.2%  lr: 0.030861  loss: 0.002405  eta: 0h6m  tot: 0h4m0s  (38.4%)93.1%  lr: 0.030731  loss: 0.002410  eta: 0h6m  tot: 0h4m1s  (38.6%)93.1%  lr: 0.030711  loss: 0.002410  eta: 0h6m  tot: 0h4m1s  (38.6%)96.1%  lr: 0.030451  loss: 0.002402  eta: 0h6m  tot: 0h4m4s  (39.2%)96.8%  lr: 0.030391  loss: 0.002405  eta: 0h6m  tot: 0h4m5s  (39.4%)97.2%  lr: 0.030351  loss: 0.002407  eta: 0h6m  tot: 0h4m5s  (39.4%)97.5%  lr: 0.030341  loss: 0.002406  eta: 0h6m  tot: 0h4m5s  (39.5%)97.6%  lr: 0.030341  loss: 0.002406  eta: 0h6m  tot: 0h4m5s  (39.5%)98.5%  lr: 0.030281  loss: 0.002408  eta: 0h6m  tot: 0h4m6s  (39.7%)99.3%  lr: 0.030241  loss: 0.002407  eta: 0h6m  tot: 0h4m7s  (39.9%)99.4%  lr: 0.030231  loss: 0.002407  eta: 0h6m  tot: 0h4m7s  (39.9%)99.6%  lr: 0.030201  loss: 0.002409  eta: 0h6m  tot: 0h4m8s  (39.9%)99.8%  lr: 0.030181  loss: 0.002408  eta: 0h6m  tot: 0h4m8s  (40.0%)\n",
      " ---+++                Epoch    1 Train error : 0.00238891 +++--- ���\n",
      "Training epoch 2: 0.03 0.01\n",
      "Epoch: 69.7%  lr: 0.023170  loss: 0.001670  eta: 0h4m  tot: 0h5m36s  (53.9%).5%  lr: 0.029940  loss: 0.001239  eta: 0h5m  tot: 0h4m11s  (40.1%)0.6%  lr: 0.029930  loss: 0.001182  eta: 0h5m  tot: 0h4m11s  (40.1%)1.5%  lr: 0.029820  loss: 0.001785  eta: 0h5m  tot: 0h4m12s  (40.3%)2.0%  lr: 0.029750  loss: 0.001745  eta: 0h6m  tot: 0h4m13s  (40.4%)2.3%  lr: 0.029710  loss: 0.001736  eta: 0h6m  tot: 0h4m14s  (40.5%)3.2%  lr: 0.029570  loss: 0.001724  eta: 0h6m  tot: 0h4m15s  (40.6%)3.7%  lr: 0.029520  loss: 0.001721  eta: 0h5m  tot: 0h4m15s  (40.7%)4.3%  lr: 0.029440  loss: 0.001701  eta: 0h5m  tot: 0h4m16s  (40.9%)6.5%  lr: 0.029230  loss: 0.001680  eta: 0h5m  tot: 0h4m18s  (41.3%)6.6%  lr: 0.029230  loss: 0.001678  eta: 0h5m  tot: 0h4m19s  (41.3%)%  lr: 0.029090  loss: 0.001684  eta: 0h5m  tot: 0h4m20s  (41.6%)8.0%  lr: 0.029080  loss: 0.001674  eta: 0h5m  tot: 0h4m20s  (41.6%)%  lr: 0.029030  loss: 0.001653  eta: 0h5m  tot: 0h4m21s  (41.7%)  lr: 0.029010  loss: 0.001662  eta: 0h5m  tot: 0h4m22s  (41.8%)9.8%  lr: 0.028960  loss: 0.001665  eta: 0h5m  tot: 0h4m23s  (42.0%)11.2%  lr: 0.028820  loss: 0.001660  eta: 0h5m  tot: 0h4m24s  (42.2%)11.6%  lr: 0.028780  loss: 0.001644  eta: 0h5m  tot: 0h4m25s  (42.3%)11.7%  lr: 0.028780  loss: 0.001646  eta: 0h5m  tot: 0h4m25s  (42.3%)14.0%  lr: 0.028640  loss: 0.001630  eta: 0h5m  tot: 0h4m28s  (42.8%)14.4%  lr: 0.028590  loss: 0.001618  eta: 0h5m  tot: 0h4m28s  (42.9%)14.6%  lr: 0.028560  loss: 0.001609  eta: 0h5m  tot: 0h4m28s  (42.9%)14.7%  lr: 0.028560  loss: 0.001621  eta: 0h5m  tot: 0h4m28s  (42.9%)14.9%  lr: 0.028530  loss: 0.001629  eta: 0h5m  tot: 0h4m29s  (43.0%)15.5%  lr: 0.028500  loss: 0.001627  eta: 0h5m  tot: 0h4m29s  (43.1%)16.1%  lr: 0.028380  loss: 0.001638  eta: 0h5m  tot: 0h4m30s  (43.2%)  loss: 0.001653  eta: 0h5m  tot: 0h4m31s  (43.3%)17.8%  lr: 0.028150  loss: 0.001652  eta: 0h5m  tot: 0h4m33s  (43.6%)18.1%  lr: 0.028120  loss: 0.001658  eta: 0h5m  tot: 0h4m33s  (43.6%)18.4%  lr: 0.028060  loss: 0.001657  eta: 0h5m  tot: 0h4m33s  (43.7%)18.7%  lr: 0.028050  loss: 0.001662  eta: 0h5m  tot: 0h4m34s  (43.7%)%  lr: 0.027950  loss: 0.001657  eta: 0h5m  tot: 0h4m35s  (44.0%)20.3%  lr: 0.027940  loss: 0.001652  eta: 0h5m  tot: 0h4m36s  (44.1%)20.7%  lr: 0.027870  loss: 0.001657  eta: 0h5m  tot: 0h4m36s  (44.1%)23.8%  lr: 0.027560  loss: 0.001669  eta: 0h5m  tot: 0h4m40s  (44.8%)24.4%  lr: 0.027490  loss: 0.001662  eta: 0h5m  tot: 0h4m40s  (44.9%)24.8%  lr: 0.027460  loss: 0.001655  eta: 0h5m  tot: 0h4m41s  (45.0%)25.1%  lr: 0.027400  loss: 0.001654  eta: 0h5m  tot: 0h4m41s  (45.0%)25.5%  lr: 0.027350  loss: 0.001657  eta: 0h5m  tot: 0h4m42s  (45.1%)25.7%  lr: 0.027340  loss: 0.001649  eta: 0h5m  tot: 0h4m42s  (45.1%)26.9%  lr: 0.027280  loss: 0.001657  eta: 0h5m  tot: 0h4m43s  (45.4%)27.1%  lr: 0.027250  loss: 0.001652  eta: 0h5m  tot: 0h4m43s  (45.4%)27.4%  lr: 0.027240  loss: 0.001648  eta: 0h5m  tot: 0h4m44s  (45.5%)27.8%  lr: 0.027200  loss: 0.001654  eta: 0h5m  tot: 0h4m44s  (45.6%)28.0%  lr: 0.027200  loss: 0.001655  eta: 0h5m  tot: 0h4m44s  (45.6%)28.3%  lr: 0.027180  loss: 0.001650  eta: 0h5m  tot: 0h4m45s  (45.7%)28.7%  lr: 0.027140  loss: 0.001650  eta: 0h5m  tot: 0h4m45s  (45.7%)29.7%  lr: 0.027090  loss: 0.001657  eta: 0h5m  tot: 0h4m46s  (45.9%)29.8%  lr: 0.027090  loss: 0.001659  eta: 0h5m  tot: 0h4m46s  (46.0%)30.4%  lr: 0.027070  loss: 0.001659  eta: 0h5m  tot: 0h4m47s  (46.1%)30.7%  lr: 0.027050  loss: 0.001656  eta: 0h5m  tot: 0h4m47s  (46.1%)31.2%  lr: 0.027030  loss: 0.001646  eta: 0h5m  tot: 0h4m48s  (46.2%)32.8%  lr: 0.026860  loss: 0.001642  eta: 0h5m  tot: 0h4m50s  (46.6%)32.8%  lr: 0.026850  loss: 0.001645  eta: 0h5m  tot: 0h4m50s  (46.6%)46.6%)33.5%  lr: 0.026790  loss: 0.001643  eta: 0h5m  tot: 0h4m51s  (46.7%)34.5%  lr: 0.026680  loss: 0.001647  eta: 0h5m  tot: 0h4m53s  (46.9%)34.6%  lr: 0.026680  loss: 0.001652  eta: 0h5m  tot: 0h4m53s  (46.9%)34.7%  lr: 0.026640  loss: 0.001651  eta: 0h5m  tot: 0h4m53s  (46.9%)34.8%  lr: 0.026620  loss: 0.001650  eta: 0h5m  tot: 0h4m53s  (47.0%)35.7%  lr: 0.026550  loss: 0.001648  eta: 0h5m  tot: 0h4m54s  (47.1%)%  lr: 0.026510  loss: 0.001641  eta: 0h5m  tot: 0h4m55s  (47.2%)36.4%  lr: 0.026470  loss: 0.001642  eta: 0h5m  tot: 0h4m55s  (47.3%)36.6%  lr: 0.026430  loss: 0.001640  eta: 0h5m  tot: 0h4m55s  (47.3%)37.3%  lr: 0.026410  loss: 0.001635  eta: 0h5m  tot: 0h4m56s  (47.5%)38.4%  lr: 0.026310  loss: 0.001645  eta: 0h5m  tot: 0h4m58s  (47.7%)39.3%  lr: 0.026180  loss: 0.001640  eta: 0h5m  tot: 0h4m59s  (47.9%)39.5%  lr: 0.026130  loss: 0.001644  eta: 0h5m  tot: 0h4m59s  (47.9%)40.9%  lr: 0.026000  loss: 0.001662  eta: 0h5m  tot: 0h5m0s  (48.2%)41.2%  lr: 0.025970  loss: 0.001662  eta: 0h5m  tot: 0h5m1s  (48.2%)42.6%  lr: 0.025870  loss: 0.001668  eta: 0h5m  tot: 0h5m2s  (48.5%)42.8%  lr: 0.025840  loss: 0.001674  eta: 0h5m  tot: 0h5m3s  (48.6%)%  lr: 0.025830  loss: 0.001680  eta: 0h5m  tot: 0h5m3s  (48.6%)43.6%  lr: 0.025820  loss: 0.001675  eta: 0h5m  tot: 0h5m3s  (48.7%)44.0%  lr: 0.025760  loss: 0.001678  eta: 0h5m  tot: 0h5m4s  (48.8%)44.5%  lr: 0.025720  loss: 0.001673  eta: 0h5m  tot: 0h5m5s  (48.9%)45.3%  lr: 0.025650  loss: 0.001674  eta: 0h5m  tot: 0h5m6s  (49.1%)46.3%  lr: 0.025580  loss: 0.001671  eta: 0h5m  tot: 0h5m7s  (49.3%)46.4%  lr: 0.025550  loss: 0.001677  eta: 0h5m  tot: 0h5m7s  (49.3%)46.5%  lr: 0.025540  loss: 0.001676  eta: 0h5m  tot: 0h5m7s  (49.3%)47.2%  lr: 0.025480  loss: 0.001680  eta: 0h5m  tot: 0h5m8s  (49.4%)48.1%  lr: 0.025370  loss: 0.001684  eta: 0h5m  tot: 0h5m9s  (49.6%)48.3%  lr: 0.025360  loss: 0.001683  eta: 0h5m  tot: 0h5m10s  (49.7%)48.5%  lr: 0.025310  loss: 0.001681  eta: 0h5m  tot: 0h5m10s  (49.7%)0.001681  eta: 0h5m  tot: 0h5m10s  (49.8%)49.7%  lr: 0.025160  loss: 0.001680  eta: 0h5m  tot: 0h5m12s  (49.9%)%  lr: 0.025130  loss: 0.001683  eta: 0h5m  tot: 0h5m12s  (50.0%)50.6%  lr: 0.025050  loss: 0.001680  eta: 0h5m  tot: 0h5m13s  (50.1%)50.8%  lr: 0.025010  loss: 0.001677  eta: 0h5m  tot: 0h5m13s  (50.2%)51.9%  lr: 0.024920  loss: 0.001671  eta: 0h5m  tot: 0h5m14s  (50.4%)52.6%  lr: 0.024800  loss: 0.001674  eta: 0h5m  tot: 0h5m15s  (50.5%)52.7%  lr: 0.024790  loss: 0.001673  eta: 0h5m  tot: 0h5m15s  (50.5%)53.5%  lr: 0.024730  loss: 0.001675  eta: 0h5m  tot: 0h5m16s  (50.7%)54.0%  lr: 0.024670  loss: 0.001678  eta: 0h5m  tot: 0h5m17s  (50.8%)  lr: 0.024640  loss: 0.001679  eta: 0h5m  tot: 0h5m17s  (50.8%)54.3%  lr: 0.024640  loss: 0.001678  eta: 0h5m  tot: 0h5m17s  (50.9%)54.5%  lr: 0.024630  loss: 0.001673  eta: 0h5m  tot: 0h5m17s  (50.9%)%  lr: 0.024570  loss: 0.001673  eta: 0h4m  tot: 0h5m18s  (51.0%)55.8%  lr: 0.024530  loss: 0.001668  eta: 0h4m  tot: 0h5m19s  (51.2%)56.2%  lr: 0.024480  loss: 0.001670  eta: 0h4m  tot: 0h5m19s  (51.2%)56.7%  lr: 0.024390  loss: 0.001669  eta: 0h4m  tot: 0h5m20s  (51.3%)57.1%  lr: 0.024360  loss: 0.001669  eta: 0h4m  tot: 0h5m20s  (51.4%)57.8%  lr: 0.024280  loss: 0.001667  eta: 0h4m  tot: 0h5m21s  (51.6%)%  lr: 0.024250  loss: 0.001667  eta: 0h4m  tot: 0h5m21s  (51.6%)%  lr: 0.024250  loss: 0.001667  eta: 0h4m  tot: 0h5m21s  (51.6%)58.4%  lr: 0.024230  loss: 0.001665  eta: 0h4m  tot: 0h5m22s  (51.7%)59.2%  lr: 0.024200  loss: 0.001665  eta: 0h4m  tot: 0h5m23s  (51.8%)59.3%  lr: 0.024200  loss: 0.001665  eta: 0h4m  tot: 0h5m23s  (51.9%)5m23s  (51.9%)0h4m  tot: 0h5m24s  (52.0%)60.0%  lr: 0.024150  loss: 0.001674  eta: 0h4m  tot: 0h5m24s  (52.0%)4m  tot: 0h5m24s  (52.1%)60.9%  lr: 0.024100  loss: 0.001670  eta: 0h4m  tot: 0h5m25s  (52.2%)h5m27s  (52.4%)  eta: 0h4m  tot: 0h5m27s  (52.4%)%  lr: 0.024010  loss: 0.001667  eta: 0h4m  tot: 0h5m27s  (52.4%)62.8%  lr: 0.023960  loss: 0.001664  eta: 0h4m  tot: 0h5m28s  (52.6%)64.1%  lr: 0.023800  loss: 0.001666  eta: 0h4m  tot: 0h5m29s  (52.8%)66.1%  lr: 0.023470  loss: 0.001674  eta: 0h4m  tot: 0h5m32s  (53.2%)66.5%  lr: 0.023430  loss: 0.001672  eta: 0h4m  tot: 0h5m33s  (53.3%)66.7%  lr: 0.023430  loss: 0.001670  eta: 0h4m  tot: 0h5m33s  (53.3%)66.8%  lr: 0.023410  loss: 0.001670  eta: 0h4m  tot: 0h5m33s  (53.4%)  lr: 0.023340  loss: 0.001675  eta: 0h4m  tot: 0h5m34s  (53.5%)68.2%  lr: 0.023300  loss: 0.001674  eta: 0h4m  tot: 0h5m34s  (53.6%)69.7%  lr: 0.023160  loss: 0.001670  eta: 0h4m  tot: 0h5m36s  (53.9%)Epoch: 100.0%  lr: 0.020279  loss: 0.001663  eta: 0h4m  tot: 0h6m13s  (60.0%)0.1%  lr: 0.023140  loss: 0.001671  eta: 0h4m  tot: 0h5m37s  (54.0%)70.4%  lr: 0.023120  loss: 0.001674  eta: 0h4m  tot: 0h5m37s  (54.1%)71.4%  lr: 0.023020  loss: 0.001678  eta: 0h4m  tot: 0h5m38s  (54.3%)72.8%  lr: 0.022900  loss: 0.001681  eta: 0h4m  tot: 0h5m40s  (54.6%)72.9%  lr: 0.022880  loss: 0.001680  eta: 0h4m  tot: 0h5m40s  (54.6%)73.2%  lr: 0.022840  loss: 0.001682  eta: 0h4m  tot: 0h5m41s  (54.6%)73.6%  lr: 0.022820  loss: 0.001683  eta: 0h4m  tot: 0h5m41s  (54.7%)73.8%  lr: 0.022810  loss: 0.001682  eta: 0h4m  tot: 0h5m41s  (54.8%)74.5%  lr: 0.022710  loss: 0.001679  eta: 0h4m  tot: 0h5m42s  (54.9%)75.1%  lr: 0.022600  loss: 0.001677  eta: 0h4m  tot: 0h5m43s  (55.0%)75.4%  lr: 0.022580  loss: 0.001674  eta: 0h4m  tot: 0h5m43s  (55.1%)75.8%  lr: 0.022530  loss: 0.001675  eta: 0h4m  tot: 0h5m44s  (55.2%)76.2%  lr: 0.022450  loss: 0.001676  eta: 0h4m  tot: 0h5m44s  (55.2%)76.8%  lr: 0.022380  loss: 0.001676  eta: 0h4m  tot: 0h5m45s  (55.4%)76.9%  lr: 0.022350  loss: 0.001677  eta: 0h4m  tot: 0h5m45s  (55.4%)77.3%  lr: 0.022290  loss: 0.001677  eta: 0h4m  tot: 0h5m46s  (55.5%)77.8%  lr: 0.022260  loss: 0.001674  eta: 0h4m  tot: 0h5m47s  (55.6%)77.8%  lr: 0.022260  loss: 0.001675  eta: 0h4m  tot: 0h5m47s  (55.6%)79.1%  lr: 0.022120  loss: 0.001671  eta: 0h4m  tot: 0h5m48s  (55.8%)79.2%  lr: 0.022110  loss: 0.001671  eta: 0h4m  tot: 0h5m48s  (55.8%)79.6%  lr: 0.022040  loss: 0.001668  eta: 0h4m  tot: 0h5m49s  (55.9%)80.2%  lr: 0.021960  loss: 0.001672  eta: 0h4m  tot: 0h5m49s  (56.0%)%  lr: 0.021760  loss: 0.001674  eta: 0h4m  tot: 0h5m53s  (56.5%)82.8%  lr: 0.021710  loss: 0.001671  eta: 0h4m  tot: 0h5m53s  (56.6%)83.2%  lr: 0.021700  loss: 0.001671  eta: 0h4m  tot: 0h5m54s  (56.6%)83.4%  lr: 0.021650  loss: 0.001671  eta: 0h4m  tot: 0h5m54s  (56.7%)83.5%  lr: 0.021630  loss: 0.001670  eta: 0h4m  tot: 0h5m54s  (56.7%)84.4%  lr: 0.021570  loss: 0.001668  eta: 0h4m  tot: 0h5m55s  (56.9%)84.7%  lr: 0.021560  loss: 0.001668  eta: 0h4m  tot: 0h5m55s  (56.9%)85.7%  lr: 0.021480  loss: 0.001673  eta: 0h4m  tot: 0h5m56s  (57.1%)85.8%  lr: 0.021480  loss: 0.001672  eta: 0h4m  tot: 0h5m56s  (57.2%) (57.3%)86.6%  lr: 0.021430  loss: 0.001673  eta: 0h4m  tot: 0h5m57s  (57.3%)87.3%  lr: 0.021390  loss: 0.001673  eta: 0h4m  tot: 0h5m58s  (57.5%)87.6%  lr: 0.021390  loss: 0.001673  eta: 0h4m  tot: 0h5m58s  (57.5%)87.7%  lr: 0.021370  loss: 0.001674  eta: 0h4m  tot: 0h5m58s  (57.5%)88.0%  lr: 0.021340  loss: 0.001674  eta: 0h4m  tot: 0h5m59s  (57.6%)88.3%  lr: 0.021310  loss: 0.001676  eta: 0h4m  tot: 0h5m59s  (57.7%)88.6%  lr: 0.021290  loss: 0.001676  eta: 0h4m  tot: 0h5m59s  (57.7%)89.5%  lr: 0.021200  loss: 0.001677  eta: 0h4m  tot: 0h6m0s  (57.9%)89.8%  lr: 0.021180  loss: 0.001675  eta: 0h4m  tot: 0h6m1s  (58.0%)90.4%  lr: 0.021140  loss: 0.001674  eta: 0h4m  tot: 0h6m1s  (58.1%)90.7%  lr: 0.021100  loss: 0.001677  eta: 0h4m  tot: 0h6m2s  (58.1%)91.3%  lr: 0.021050  loss: 0.001677  eta: 0h4m  tot: 0h6m2s  (58.3%)92.6%  lr: 0.020990  loss: 0.001677  eta: 0h4m  tot: 0h6m4s  (58.5%)93.7%  lr: 0.020920  loss: 0.001672  eta: 0h4m  tot: 0h6m5s  (58.7%)94.3%  lr: 0.020860  loss: 0.001671  eta: 0h4m  tot: 0h6m6s  (58.9%)94.5%  lr: 0.020840  loss: 0.001670  eta: 0h4m  tot: 0h6m6s  (58.9%)95.0%  lr: 0.020820  loss: 0.001672  eta: 0h4m  tot: 0h6m7s  (59.0%)95.7%  lr: 0.020790  loss: 0.001671  eta: 0h4m  tot: 0h6m7s  (59.1%)96.6%  lr: 0.020629  loss: 0.001667  eta: 0h4m  tot: 0h6m9s  (59.3%)96.8%  lr: 0.020619  loss: 0.001666  eta: 0h4m  tot: 0h6m9s  (59.4%)97.1%  lr: 0.020589  loss: 0.001666  eta: 0h4m  tot: 0h6m9s  (59.4%)%  lr: 0.020479  loss: 0.001668  eta: 0h4m  tot: 0h6m10s  (59.6%)99.1%  lr: 0.020329  loss: 0.001667  eta: 0h4m  tot: 0h6m12s  (59.8%)99.4%  lr: 0.020309  loss: 0.001666  eta: 0h4m  tot: 0h6m12s  (59.9%)  lr: 0.020309  loss: 0.001665  eta: 0h4m  tot: 0h6m12s  (59.9%)99.8%  lr: 0.020289  loss: 0.001664  eta: 0h4m  tot: 0h6m13s  (60.0%)\n",
      " ---+++                Epoch    2 Train error : 0.00166470 +++--- ���\n",
      "Training epoch 3: 0.02 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58.5%  lr: 0.014080  loss: 0.001311  eta: 0h2m  tot: 0h7m21s  (71.7%).2%  lr: 0.019980  loss: 0.000892  eta: 0h4m  tot: 0h6m16s  (60.0%)0.8%  lr: 0.019920  loss: 0.000927  eta: 0h3m  tot: 0h6m17s  (60.2%)0.9%  lr: 0.019910  loss: 0.000988  eta: 0h3m  tot: 0h6m17s  (60.2%)1.3%  lr: 0.019890  loss: 0.001040  eta: 0h3m  tot: 0h6m18s  (60.3%)1.4%  lr: 0.019870  loss: 0.001100  eta: 0h3m  tot: 0h6m18s  (60.3%)1.7%  lr: 0.019830  loss: 0.001087  eta: 0h3m  tot: 0h6m18s  (60.3%)2.1%  lr: 0.019770  loss: 0.001086  eta: 0h3m  tot: 0h6m19s  (60.4%)2.7%  lr: 0.019750  loss: 0.001205  eta: 0h3m  tot: 0h6m19s  (60.5%)3.0%  lr: 0.019730  loss: 0.001274  eta: 0h3m  tot: 0h6m20s  (60.6%)3.3%  lr: 0.019710  loss: 0.001357  eta: 0h3m  tot: 0h6m20s  (60.7%)3.7%  lr: 0.019670  loss: 0.001402  eta: 0h3m  tot: 0h6m20s  (60.7%)4.5%  lr: 0.019560  loss: 0.001434  eta: 0h3m  tot: 0h6m21s  (60.9%)5.1%  lr: 0.019530  loss: 0.001387  eta: 0h3m  tot: 0h6m22s  (61.0%)5.5%  lr: 0.019490  loss: 0.001377  eta: 0h3m  tot: 0h6m22s  (61.1%)6.1%  lr: 0.019420  loss: 0.001354  eta: 0h3m  tot: 0h6m23s  (61.2%)7.4%  lr: 0.019320  loss: 0.001322  eta: 0h3m  tot: 0h6m24s  (61.5%)7.6%  lr: 0.019300  loss: 0.001317  eta: 0h3m  tot: 0h6m25s  (61.5%)7.8%  lr: 0.019290  loss: 0.001305  eta: 0h3m  tot: 0h6m25s  (61.6%)7.9%  lr: 0.019280  loss: 0.001303  eta: 0h3m  tot: 0h6m25s  (61.6%)8.6%  lr: 0.019200  loss: 0.001302  eta: 0h3m  tot: 0h6m26s  (61.7%)9.3%  lr: 0.019170  loss: 0.001330  eta: 0h3m  tot: 0h6m26s  (61.9%)9.9%  lr: 0.019120  loss: 0.001302  eta: 0h3m  tot: 0h6m27s  (62.0%)10.0%  lr: 0.019100  loss: 0.001306  eta: 0h3m  tot: 0h6m27s  (62.0%)10.5%  lr: 0.019070  loss: 0.001338  eta: 0h3m  tot: 0h6m28s  (62.1%)10.8%  lr: 0.019050  loss: 0.001333  eta: 0h3m  tot: 0h6m28s  (62.2%)11.2%  lr: 0.018990  loss: 0.001335  eta: 0h3m  tot: 0h6m28s  (62.2%)11.4%  lr: 0.018960  loss: 0.001329  eta: 0h3m  tot: 0h6m29s  (62.3%)11.6%  lr: 0.018960  loss: 0.001324  eta: 0h3m  tot: 0h6m29s  (62.3%)12.7%  lr: 0.018850  loss: 0.001326  eta: 0h3m  tot: 0h6m30s  (62.5%)13.2%  lr: 0.018790  loss: 0.001324  eta: 0h3m  tot: 0h6m31s  (62.6%)13.6%  lr: 0.018730  loss: 0.001318  eta: 0h3m  tot: 0h6m31s  (62.7%)14.0%  lr: 0.018670  loss: 0.001311  eta: 0h3m  tot: 0h6m32s  (62.8%)14.6%  lr: 0.018610  loss: 0.001325  eta: 0h3m  tot: 0h6m32s  (62.9%)15.7%  lr: 0.018480  loss: 0.001340  eta: 0h3m  tot: 0h6m34s  (63.1%)16.0%  lr: 0.018460  loss: 0.001339  eta: 0h3m  tot: 0h6m34s  (63.2%)16.7%  lr: 0.018360  loss: 0.001343  eta: 0h3m  tot: 0h6m35s  (63.3%)17.0%  lr: 0.018310  loss: 0.001345  eta: 0h3m  tot: 0h6m35s  (63.4%)17.6%  lr: 0.018190  loss: 0.001349  eta: 0h3m  tot: 0h6m36s  (63.5%)18.5%  lr: 0.018120  loss: 0.001337  eta: 0h3m  tot: 0h6m37s  (63.7%)19.2%  lr: 0.018060  loss: 0.001331  eta: 0h3m  tot: 0h6m38s  (63.8%)19.8%  lr: 0.017960  loss: 0.001347  eta: 0h3m  tot: 0h6m39s  (64.0%)19.9%  lr: 0.017960  loss: 0.001347  eta: 0h3m  tot: 0h6m39s  (64.0%)20.1%  lr: 0.017960  loss: 0.001345  eta: 0h3m  tot: 0h6m39s  (64.0%)20.6%  lr: 0.017920  loss: 0.001355  eta: 0h3m  tot: 0h6m39s  (64.1%)20.9%  lr: 0.017900  loss: 0.001358  eta: 0h3m  tot: 0h6m40s  (64.2%)21.2%  lr: 0.017850  loss: 0.001355  eta: 0h3m  tot: 0h6m40s  (64.2%)21.3%  lr: 0.017830  loss: 0.001359  eta: 0h3m  tot: 0h6m40s  (64.3%)21.4%  lr: 0.017820  loss: 0.001355  eta: 0h3m  tot: 0h6m40s  (64.3%)21.7%  lr: 0.017790  loss: 0.001349  eta: 0h3m  tot: 0h6m41s  (64.3%)22.0%  lr: 0.017770  loss: 0.001348  eta: 0h3m  tot: 0h6m41s  (64.4%)23.1%  lr: 0.017660  loss: 0.001343  eta: 0h3m  tot: 0h6m43s  (64.6%)23.2%  lr: 0.017650  loss: 0.001341  eta: 0h3m  tot: 0h6m43s  (64.6%)24.5%  lr: 0.017520  loss: 0.001356  eta: 0h3m  tot: 0h6m44s  (64.9%)24.7%  lr: 0.017490  loss: 0.001347  eta: 0h3m  tot: 0h6m44s  (64.9%)24.9%  lr: 0.017450  loss: 0.001343  eta: 0h3m  tot: 0h6m45s  (65.0%)25.1%  lr: 0.017420  loss: 0.001341  eta: 0h3m  tot: 0h6m45s  (65.0%)25.3%  lr: 0.017380  loss: 0.001342  eta: 0h3m  tot: 0h6m45s  (65.1%)25.8%  lr: 0.017360  loss: 0.001332  eta: 0h3m  tot: 0h6m45s  (65.2%)26.0%  lr: 0.017330  loss: 0.001327  eta: 0h3m  tot: 0h6m46s  (65.2%)26.2%  lr: 0.017310  loss: 0.001321  eta: 0h3m  tot: 0h6m46s  (65.2%)26.7%  lr: 0.017260  loss: 0.001322  eta: 0h3m  tot: 0h6m46s  (65.3%)27.5%  lr: 0.017170  loss: 0.001322  eta: 0h3m  tot: 0h6m47s  (65.5%)28.1%  lr: 0.017140  loss: 0.001317  eta: 0h3m  tot: 0h6m48s  (65.6%)28.6%  lr: 0.017080  loss: 0.001317  eta: 0h3m  tot: 0h6m48s  (65.7%)29.6%  lr: 0.017000  loss: 0.001309  eta: 0h3m  tot: 0h6m49s  (65.9%)30.5%  lr: 0.016900  loss: 0.001316  eta: 0h3m  tot: 0h6m51s  (66.1%)30.6%  lr: 0.016890  loss: 0.001313  eta: 0h3m  tot: 0h6m51s  (66.1%)  tot: 0h6m51s  (66.2%)31.0%  lr: 0.016840  loss: 0.001309  eta: 0h3m  tot: 0h6m51s  (66.2%)31.8%  lr: 0.016730  loss: 0.001317  eta: 0h3m  tot: 0h6m52s  (66.4%)32.0%  lr: 0.016700  loss: 0.001321  eta: 0h3m  tot: 0h6m52s  (66.4%)34.6%  lr: 0.016480  loss: 0.001309  eta: 0h3m  tot: 0h6m55s  (66.9%)35.1%  lr: 0.016450  loss: 0.001316  eta: 0h3m  tot: 0h6m56s  (67.0%)35.3%  lr: 0.016450  loss: 0.001314  eta: 0h3m  tot: 0h6m56s  (67.1%)35.5%  lr: 0.016400  loss: 0.001311  eta: 0h3m  tot: 0h6m56s  (67.1%)%  lr: 0.016200  loss: 0.001301  eta: 0h3m  tot: 0h6m58s  (67.4%)37.8%  lr: 0.016090  loss: 0.001298  eta: 0h3m  tot: 0h6m58s  (67.6%)38.2%  lr: 0.016060  loss: 0.001296  eta: 0h3m  tot: 0h6m59s  (67.6%)38.6%  lr: 0.016010  loss: 0.001290  eta: 0h3m  tot: 0h6m59s  (67.7%)39.1%  lr: 0.015930  loss: 0.001296  eta: 0h2m  tot: 0h7m0s  (67.8%)39.5%  lr: 0.015880  loss: 0.001293  eta: 0h2m  tot: 0h7m1s  (67.9%)39.9%  lr: 0.015820  loss: 0.001290  eta: 0h2m  tot: 0h7m1s  (68.0%)40.9%  lr: 0.015720  loss: 0.001299  eta: 0h2m  tot: 0h7m2s  (68.2%)41.7%  lr: 0.015610  loss: 0.001303  eta: 0h2m  tot: 0h7m3s  (68.3%)%  lr: 0.015510  loss: 0.001311  eta: 0h2m  tot: 0h7m4s  (68.5%)43.4%  lr: 0.015460  loss: 0.001315  eta: 0h2m  tot: 0h7m5s  (68.7%)43.6%  lr: 0.015450  loss: 0.001315  eta: 0h2m  tot: 0h7m5s  (68.7%)43.8%  lr: 0.015410  loss: 0.001311  eta: 0h2m  tot: 0h7m5s  (68.8%)44.2%  lr: 0.015380  loss: 0.001310  eta: 0h2m  tot: 0h7m6s  (68.8%)44.6%  lr: 0.015350  loss: 0.001309  eta: 0h2m  tot: 0h7m6s  (68.9%)45.8%  lr: 0.015220  loss: 0.001309  eta: 0h2m  tot: 0h7m7s  (69.2%)46.8%  lr: 0.015130  loss: 0.001313  eta: 0h2m  tot: 0h7m8s  (69.4%)47.1%  lr: 0.015110  loss: 0.001308  eta: 0h2m  tot: 0h7m9s  (69.4%)47.2%  lr: 0.015090  loss: 0.001306  eta: 0h2m  tot: 0h7m9s  (69.4%)47.8%  lr: 0.015070  loss: 0.001308  eta: 0h2m  tot: 0h7m9s  (69.6%)48.2%  lr: 0.015040  loss: 0.001306  eta: 0h2m  tot: 0h7m10s  (69.6%)48.6%  lr: 0.014990  loss: 0.001312  eta: 0h2m  tot: 0h7m10s  (69.7%)48.9%  lr: 0.014960  loss: 0.001314  eta: 0h2m  tot: 0h7m11s  (69.8%)%  lr: 0.014870  loss: 0.001307  eta: 0h2m  tot: 0h7m12s  (70.1%)50.5%  lr: 0.014850  loss: 0.001306  eta: 0h2m  tot: 0h7m13s  (70.1%)50.8%  lr: 0.014850  loss: 0.001307  eta: 0h2m  tot: 0h7m13s  (70.2%)51.1%  lr: 0.014810  loss: 0.001304  eta: 0h2m  tot: 0h7m13s  (70.2%)51.4%  lr: 0.014780  loss: 0.001303  eta: 0h2m  tot: 0h7m13s  (70.3%)51.9%  lr: 0.014720  loss: 0.001301  eta: 0h2m  tot: 0h7m14s  (70.4%)52.0%  lr: 0.014710  loss: 0.001302  eta: 0h2m  tot: 0h7m14s  (70.4%)52.2%  lr: 0.014680  loss: 0.001301  eta: 0h2m  tot: 0h7m14s  (70.4%)52.6%  lr: 0.014650  loss: 0.001300  eta: 0h2m  tot: 0h7m15s  (70.5%)53.0%  lr: 0.014580  loss: 0.001300  eta: 0h2m  tot: 0h7m15s  (70.6%)53.2%  lr: 0.014540  loss: 0.001300  eta: 0h2m  tot: 0h7m15s  (70.6%)53.3%  lr: 0.014510  loss: 0.001300  eta: 0h2m  tot: 0h7m15s  (70.7%)%  lr: 0.014460  loss: 0.001301  eta: 0h2m  tot: 0h7m16s  (70.7%)54.4%  lr: 0.014400  loss: 0.001313  eta: 0h2m  tot: 0h7m17s  (70.9%)55.6%  lr: 0.014310  loss: 0.001316  eta: 0h2m  tot: 0h7m18s  (71.1%)56.3%  lr: 0.014230  loss: 0.001315  eta: 0h2m  tot: 0h7m19s  (71.3%)56.7%  lr: 0.014220  loss: 0.001321  eta: 0h2m  tot: 0h7m19s  (71.3%)57.1%  lr: 0.014190  loss: 0.001321  eta: 0h2m  tot: 0h7m20s  (71.4%)57.1%  lr: 0.014190  loss: 0.001320  eta: 0h2m  tot: 0h7m20s  (71.4%)57.5%  lr: 0.014150  loss: 0.001316  eta: 0h2m  tot: 0h7m20s  (71.5%)58.3%  lr: 0.014100  loss: 0.001311  eta: 0h2m  tot: 0h7m21s  (71.7%)58.6%  lr: 0.014080  loss: 0.001310  eta: 0h2m  tot: 0h7m21s  (71.7%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.010190  loss: 0.001302  eta: 0h1m  tot: 0h8m7s  (80.0%)59.4%  lr: 0.014040  loss: 0.001312  eta: 0h2m  tot: 0h7m22s  (71.9%)  lr: 0.014020  loss: 0.001311  eta: 0h2m  tot: 0h7m23s  (71.9%)60.7%  lr: 0.013900  loss: 0.001308  eta: 0h2m  tot: 0h7m24s  (72.1%)61.3%  lr: 0.013850  loss: 0.001308  eta: 0h2m  tot: 0h7m24s  (72.3%)62.5%  lr: 0.013770  loss: 0.001306  eta: 0h2m  tot: 0h7m26s  (72.5%)62.8%  lr: 0.013740  loss: 0.001310  eta: 0h2m  tot: 0h7m26s  (72.6%)63.7%  lr: 0.013630  loss: 0.001306  eta: 0h2m  tot: 0h7m27s  (72.7%)64.2%  lr: 0.013570  loss: 0.001306  eta: 0h2m  tot: 0h7m27s  (72.8%)64.4%  lr: 0.013520  loss: 0.001304  eta: 0h2m  tot: 0h7m28s  (72.9%)64.6%  lr: 0.013510  loss: 0.001305  eta: 0h2m  tot: 0h7m28s  (72.9%)65.2%  lr: 0.013450  loss: 0.001304  eta: 0h2m  tot: 0h7m29s  (73.0%)65.5%  lr: 0.013400  loss: 0.001304  eta: 0h2m  tot: 0h7m29s  (73.1%)66.0%  lr: 0.013350  loss: 0.001303  eta: 0h2m  tot: 0h7m29s  (73.2%)66.2%  lr: 0.013350  loss: 0.001306  eta: 0h2m  tot: 0h7m30s  (73.2%)66.9%  lr: 0.013270  loss: 0.001307  eta: 0h2m  tot: 0h7m30s  (73.4%)73.4%)67.1%  lr: 0.013250  loss: 0.001305  eta: 0h2m  tot: 0h7m31s  (73.4%)67.8%  lr: 0.013210  loss: 0.001305  eta: 0h2m  tot: 0h7m31s  (73.6%)68.8%  lr: 0.013120  loss: 0.001308  eta: 0h2m  tot: 0h7m33s  (73.8%)69.1%  lr: 0.013080  loss: 0.001311  eta: 0h2m  tot: 0h7m33s  (73.8%)69.2%  lr: 0.013030  loss: 0.001311  eta: 0h2m  tot: 0h7m33s  (73.8%)69.9%  lr: 0.012960  loss: 0.001311  eta: 0h2m  tot: 0h7m34s  (74.0%)70.6%  lr: 0.012890  loss: 0.001309  eta: 0h2m  tot: 0h7m35s  (74.1%)70.7%  lr: 0.012870  loss: 0.001309  eta: 0h2m  tot: 0h7m35s  (74.1%)72.1%  lr: 0.012720  loss: 0.001304  eta: 0h2m  tot: 0h7m36s  (74.4%)72.4%  lr: 0.012690  loss: 0.001304  eta: 0h2m  tot: 0h7m37s  (74.5%)72.6%  lr: 0.012680  loss: 0.001304  eta: 0h2m  tot: 0h7m37s  (74.5%)72.8%  lr: 0.012660  loss: 0.001304  eta: 0h2m  tot: 0h7m37s  (74.6%)73.0%  lr: 0.012650  loss: 0.001303  eta: 0h2m  tot: 0h7m37s  (74.6%)73.2%  lr: 0.012630  loss: 0.001303  eta: 0h2m  tot: 0h7m38s  (74.6%)73.3%  lr: 0.012630  loss: 0.001303  eta: 0h2m  tot: 0h7m38s  (74.7%)73.3%  lr: 0.012620  loss: 0.001302  eta: 0h2m  tot: 0h7m38s  (74.7%)73.4%  lr: 0.012600  loss: 0.001302  eta: 0h2m  tot: 0h7m38s  (74.7%)74.0%  lr: 0.012590  loss: 0.001302  eta: 0h2m  tot: 0h7m38s  (74.8%)74.3%  lr: 0.012570  loss: 0.001304  eta: 0h2m  tot: 0h7m39s  (74.9%)74.7%  lr: 0.012530  loss: 0.001302  eta: 0h2m  tot: 0h7m39s  (74.9%)75.0%  lr: 0.012520  loss: 0.001300  eta: 0h2m  tot: 0h7m39s  (75.0%)75.1%  lr: 0.012510  loss: 0.001299  eta: 0h2m  tot: 0h7m39s  (75.0%)75.5%  lr: 0.012450  loss: 0.001300  eta: 0h2m  tot: 0h7m40s  (75.1%)76.0%  lr: 0.012370  loss: 0.001300  eta: 0h2m  tot: 0h7m40s  (75.2%)76.0%  lr: 0.012370  loss: 0.001298  eta: 0h2m  tot: 0h7m40s  (75.2%)76.1%  lr: 0.012340  loss: 0.001297  eta: 0h2m  tot: 0h7m41s  (75.2%)77.0%  lr: 0.012280  loss: 0.001296  eta: 0h2m  tot: 0h7m42s  (75.4%)77.1%  lr: 0.012280  loss: 0.001295  eta: 0h2m  tot: 0h7m42s  (75.4%)77.4%  lr: 0.012270  loss: 0.001294  eta: 0h2m  tot: 0h7m42s  (75.5%)2m  tot: 0h7m42s  (75.5%)77.9%  lr: 0.012210  loss: 0.001294  eta: 0h2m  tot: 0h7m42s  (75.6%)80.4%  lr: 0.011940  loss: 0.001302  eta: 0h2m  tot: 0h7m45s  (76.1%)80.5%  lr: 0.011910  loss: 0.001303  eta: 0h2m  tot: 0h7m45s  (76.1%)80.6%  lr: 0.011900  loss: 0.001304  eta: 0h2m  tot: 0h7m45s  (76.1%)82.3%  lr: 0.011750  loss: 0.001310  eta: 0h2m  tot: 0h7m47s  (76.5%)82.4%  lr: 0.011740  loss: 0.001308  eta: 0h2m  tot: 0h7m47s  (76.5%)83.4%  lr: 0.011660  loss: 0.001309  eta: 0h2m  tot: 0h7m48s  (76.7%)84.1%  lr: 0.011620  loss: 0.001307  eta: 0h2m  tot: 0h7m49s  (76.8%)84.3%  lr: 0.011600  loss: 0.001307  eta: 0h2m  tot: 0h7m49s  (76.9%)85.2%  lr: 0.011510  loss: 0.001307  eta: 0h2m  tot: 0h7m50s  (77.0%)85.9%  lr: 0.011430  loss: 0.001309  eta: 0h2m  tot: 0h7m51s  (77.2%)86.2%  lr: 0.011400  loss: 0.001308  eta: 0h2m  tot: 0h7m52s  (77.2%)87.1%  lr: 0.011330  loss: 0.001307  eta: 0h2m  tot: 0h7m52s  (77.4%)87.4%  lr: 0.011330  loss: 0.001306  eta: 0h2m  tot: 0h7m53s  (77.5%)87.9%  lr: 0.011280  loss: 0.001308  eta: 0h2m  tot: 0h7m53s  (77.6%)88.3%  lr: 0.011250  loss: 0.001305  eta: 0h2m  tot: 0h7m54s  (77.7%)88.4%  lr: 0.011240  loss: 0.001305  eta: 0h2m  tot: 0h7m54s  (77.7%)88.6%  lr: 0.011220  loss: 0.001303  eta: 0h2m  tot: 0h7m54s  (77.7%)88.6%  lr: 0.011220  loss: 0.001303  eta: 0h2m  tot: 0h7m54s  (77.7%)88.9%  lr: 0.011220  loss: 0.001303  eta: 0h2m  tot: 0h7m54s  (77.8%)89.0%  lr: 0.011220  loss: 0.001304  eta: 0h2m  tot: 0h7m54s  (77.8%)89.8%  lr: 0.011150  loss: 0.001308  eta: 0h2m  tot: 0h7m55s  (78.0%)90.0%  lr: 0.011140  loss: 0.001308  eta: 0h2m  tot: 0h7m55s  (78.0%)90.1%  lr: 0.011120  loss: 0.001308  eta: 0h2m  tot: 0h7m56s  (78.0%)90.4%  lr: 0.011070  loss: 0.001307  eta: 0h2m  tot: 0h7m56s  (78.1%)91.1%  lr: 0.011000  loss: 0.001307  eta: 0h2m  tot: 0h7m57s  (78.2%)91.9%  lr: 0.010890  loss: 0.001308  eta: 0h1m  tot: 0h7m58s  (78.4%)92.2%  lr: 0.010870  loss: 0.001307  eta: 0h1m  tot: 0h7m58s  (78.4%)92.5%  lr: 0.010850  loss: 0.001307  eta: 0h1m  tot: 0h7m58s  (78.5%)92.6%  lr: 0.010840  loss: 0.001307  eta: 0h1m  tot: 0h7m58s  (78.5%)93.6%  lr: 0.010780  loss: 0.001306  eta: 0h1m  tot: 0h7m59s  (78.7%)94.4%  lr: 0.010690  loss: 0.001306  eta: 0h1m  tot: 0h8m0s  (78.9%)%  lr: 0.010690  loss: 0.001305  eta: 0h1m  tot: 0h8m0s  (78.9%)96.3%  lr: 0.010460  loss: 0.001305  eta: 0h1m  tot: 0h8m3s  (79.3%)96.7%  lr: 0.010410  loss: 0.001306  eta: 0h1m  tot: 0h8m3s  (79.3%)96.7%  lr: 0.010410  loss: 0.001306  eta: 0h1m  tot: 0h8m3s  (79.3%)97.0%  lr: 0.010370  loss: 0.001308  eta: 0h1m  tot: 0h8m4s  (79.4%)97.1%  lr: 0.010360  loss: 0.001308  eta: 0h1m  tot: 0h8m4s  (79.4%)97.3%  lr: 0.010350  loss: 0.001308  eta: 0h1m  tot: 0h8m4s  (79.5%)98.5%  lr: 0.010280  loss: 0.001303  eta: 0h1m  tot: 0h8m6s  (79.7%)99.2%  lr: 0.010230  loss: 0.001302  eta: 0h1m  tot: 0h8m6s  (79.8%)\n",
      " ---+++                Epoch    3 Train error : 0.00133022 +++--- ���\n",
      "Training epoch 4: 0.01 0.01\n",
      "Epoch: 63.2%  lr: 0.003820  loss: 0.001161  eta: <1min   tot: 0h9m23s  (92.6%).2%  lr: 0.010000  loss: 0.001918  eta: 0h1m  tot: 0h8m10s  (80.0%)1.2%  lr: 0.009890  loss: 0.001270  eta: 0h1m  tot: 0h8m11s  (80.2%)1.8%  lr: 0.009850  loss: 0.001375  eta: 0h1m  tot: 0h8m12s  (80.4%)3.1%  lr: 0.009750  loss: 0.001240  eta: 0h1m  tot: 0h8m13s  (80.6%)3.4%  lr: 0.009700  loss: 0.001215  eta: 0h1m  tot: 0h8m14s  (80.7%)3.6%  lr: 0.009680  loss: 0.001211  eta: 0h1m  tot: 0h8m14s  (80.7%)3.8%  lr: 0.009660  loss: 0.001258  eta: 0h1m  tot: 0h8m14s  (80.8%)4.0%  lr: 0.009640  loss: 0.001238  eta: 0h1m  tot: 0h8m15s  (80.8%)4.2%  lr: 0.009600  loss: 0.001225  eta: 0h1m  tot: 0h8m15s  (80.8%)6.0%  lr: 0.009510  loss: 0.001207  eta: 0h1m  tot: 0h8m17s  (81.2%)6.4%  lr: 0.009450  loss: 0.001204  eta: 0h1m  tot: 0h8m17s  (81.3%)  eta: 0h1m  tot: 0h8m17s  (81.3%)6.8%  lr: 0.009440  loss: 0.001226  eta: 0h1m  tot: 0h8m18s  (81.4%)7.5%  lr: 0.009400  loss: 0.001217  eta: 0h1m  tot: 0h8m18s  (81.5%)7.6%  lr: 0.009400  loss: 0.001218  eta: 0h1m  tot: 0h8m19s  (81.5%)8.5%  lr: 0.009270  loss: 0.001214  eta: 0h1m  tot: 0h8m20s  (81.7%)8.8%  lr: 0.009260  loss: 0.001217  eta: 0h1m  tot: 0h8m20s  (81.8%)9.0%  lr: 0.009240  loss: 0.001238  eta: 0h1m  tot: 0h8m20s  (81.8%)9.4%  lr: 0.009220  loss: 0.001233  eta: 0h1m  tot: 0h8m21s  (81.9%)9.5%  lr: 0.009210  loss: 0.001237  eta: 0h1m  tot: 0h8m21s  (81.9%)10.3%  lr: 0.009140  loss: 0.001230  eta: 0h1m  tot: 0h8m22s  (82.1%)11.1%  lr: 0.009070  loss: 0.001233  eta: 0h1m  tot: 0h8m23s  (82.2%)11.8%  lr: 0.008980  loss: 0.001228  eta: 0h1m  tot: 0h8m23s  (82.4%)12.3%  lr: 0.008890  loss: 0.001214  eta: 0h1m  tot: 0h8m24s  (82.5%)13.7%  lr: 0.008790  loss: 0.001186  eta: 0h1m  tot: 0h8m25s  (82.7%)  lr: 0.008720  loss: 0.001183  eta: 0h1m  tot: 0h8m26s  (82.9%)15.5%  lr: 0.008590  loss: 0.001177  eta: 0h1m  tot: 0h8m28s  (83.1%)16.2%  lr: 0.008520  loss: 0.001175  eta: 0h1m  tot: 0h8m28s  (83.2%)16.8%  lr: 0.008430  loss: 0.001158  eta: 0h1m  tot: 0h8m29s  (83.4%)17.7%  lr: 0.008390  loss: 0.001156  eta: 0h1m  tot: 0h8m30s  (83.5%)18.2%  lr: 0.008370  loss: 0.001158  eta: 0h1m  tot: 0h8m30s  (83.6%)20.3%  lr: 0.008180  loss: 0.001149  eta: 0h1m  tot: 0h8m33s  (84.1%)20.6%  lr: 0.008160  loss: 0.001164  eta: 0h1m  tot: 0h8m33s  (84.1%)20.9%  lr: 0.008130  loss: 0.001163  eta: 0h1m  tot: 0h8m33s  (84.2%)21.0%  lr: 0.008100  loss: 0.001167  eta: 0h1m  tot: 0h8m33s  (84.2%)21.4%  lr: 0.008040  loss: 0.001159  eta: 0h1m  tot: 0h8m34s  (84.3%)21.6%  lr: 0.008030  loss: 0.001157  eta: 0h1m  tot: 0h8m34s  (84.3%)22.4%  lr: 0.007960  loss: 0.001151  eta: 0h1m  tot: 0h8m35s  (84.5%)23.3%  lr: 0.007900  loss: 0.001141  eta: 0h1m  tot: 0h8m36s  (84.7%)24.3%  lr: 0.007810  loss: 0.001143  eta: 0h1m  tot: 0h8m37s  (84.9%)%  lr: 0.007720  loss: 0.001147  eta: 0h1m  tot: 0h8m38s  (85.1%)25.9%  lr: 0.007620  loss: 0.001145  eta: 0h1m  tot: 0h8m39s  (85.2%)26.2%  lr: 0.007590  loss: 0.001149  eta: 0h1m  tot: 0h8m39s  (85.2%)m  tot: 0h8m40s  (85.3%)27.4%  lr: 0.007430  loss: 0.001147  eta: 0h1m  tot: 0h8m40s  (85.5%)28.2%  lr: 0.007350  loss: 0.001141  eta: 0h1m  tot: 0h8m42s  (85.6%)28.6%  lr: 0.007350  loss: 0.001144  eta: 0h1m  tot: 0h8m42s  (85.7%)28.7%  lr: 0.007350  loss: 0.001142  eta: 0h1m  tot: 0h8m42s  (85.7%)28.9%  lr: 0.007340  loss: 0.001142  eta: 0h1m  tot: 0h8m42s  (85.8%)29.0%  lr: 0.007330  loss: 0.001139  eta: 0h1m  tot: 0h8m42s  (85.8%)29.2%  lr: 0.007300  loss: 0.001141  eta: 0h1m  tot: 0h8m43s  (85.8%)30.3%  lr: 0.007200  loss: 0.001140  eta: 0h1m  tot: 0h8m44s  (86.1%)31.5%  lr: 0.007100  loss: 0.001151  eta: 0h1m  tot: 0h8m46s  (86.3%)33.0%  lr: 0.006950  loss: 0.001160  eta: 0h1m  tot: 0h8m47s  (86.6%)33.8%  lr: 0.006870  loss: 0.001160  eta: 0h1m  tot: 0h8m48s  (86.8%)86.9%)%  lr: 0.006780  loss: 0.001169  eta: 0h1m  tot: 0h8m50s  (87.0%)35.7%  lr: 0.006750  loss: 0.001167  eta: 0h1m  tot: 0h8m51s  (87.1%)35.8%  lr: 0.006720  loss: 0.001169  eta: 0h1m  tot: 0h8m51s  (87.2%)37.2%  lr: 0.006630  loss: 0.001162  eta: 0h1m  tot: 0h8m52s  (87.4%)37.6%  lr: 0.006570  loss: 0.001160  eta: 0h1m  tot: 0h8m53s  (87.5%)37.7%  lr: 0.006560  loss: 0.001162  eta: 0h1m  tot: 0h8m53s  (87.5%)38.0%  lr: 0.006520  loss: 0.001160  eta: 0h1m  tot: 0h8m54s  (87.6%)38.1%  lr: 0.006510  loss: 0.001161  eta: 0h1m  tot: 0h8m54s  (87.6%)39.3%  lr: 0.006430  loss: 0.001168  eta: 0h1m  tot: 0h8m55s  (87.9%)0.006360  loss: 0.001161  eta: 0h1m  tot: 0h8m56s  (88.0%)40.0%  lr: 0.006330  loss: 0.001162  eta: 0h1m  tot: 0h8m56s  (88.0%)40.6%  lr: 0.006250  loss: 0.001165  eta: 0h1m  tot: 0h8m57s  (88.1%)40.7%  lr: 0.006250  loss: 0.001165  eta: 0h1m  tot: 0h8m57s  (88.1%)40.9%  lr: 0.006230  loss: 0.001168  eta: 0h1m  tot: 0h8m57s  (88.2%)41.5%  lr: 0.006170  loss: 0.001169  eta: 0h1m  tot: 0h8m58s  (88.3%)42.2%  lr: 0.006100  loss: 0.001170  eta: 0h1m  tot: 0h8m59s  (88.4%)42.3%  lr: 0.006080  loss: 0.001169  eta: 0h1m  tot: 0h8m59s  (88.5%)42.4%  lr: 0.006060  loss: 0.001167  eta: 0h1m  tot: 0h8m59s  (88.5%)42.5%  lr: 0.006050  loss: 0.001169  eta: 0h1m  tot: 0h8m59s  (88.5%)42.8%  lr: 0.006010  loss: 0.001169  eta: 0h1m  tot: 0h9m0s  (88.6%)43.0%  lr: 0.006010  loss: 0.001167  eta: 0h1m  tot: 0h9m0s  (88.6%)43.5%  lr: 0.005970  loss: 0.001164  eta: 0h1m  tot: 0h9m1s  (88.7%)44.2%  lr: 0.005910  loss: 0.001158  eta: 0h1m  tot: 0h9m2s  (88.8%)45.1%  lr: 0.005850  loss: 0.001158  eta: 0h1m  tot: 0h9m3s  (89.0%)45.3%  lr: 0.005830  loss: 0.001159  eta: 0h1m  tot: 0h9m3s  (89.1%)45.6%  lr: 0.005800  loss: 0.001159  eta: 0h1m  tot: 0h9m3s  (89.1%)46.2%  lr: 0.005740  loss: 0.001156  eta: 0h1m  tot: 0h9m4s  (89.2%)46.3%  lr: 0.005740  loss: 0.001154  eta: 0h1m  tot: 0h9m4s  (89.3%)46.5%  lr: 0.005720  loss: 0.001152  eta: 0h1m  tot: 0h9m4s  (89.3%)46.6%  lr: 0.005700  loss: 0.001152  eta: 0h1m  tot: 0h9m4s  (89.3%)47.2%  lr: 0.005610  loss: 0.001159  eta: 0h1m  tot: 0h9m5s  (89.4%)47.3%  lr: 0.005570  loss: 0.001159  eta: 0h1m  tot: 0h9m5s  (89.5%)47.5%  lr: 0.005560  loss: 0.001159  eta: 0h1m  tot: 0h9m5s  (89.5%)47.7%  lr: 0.005540  loss: 0.001158  eta: 0h1m  tot: 0h9m6s  (89.5%)47.8%  lr: 0.005520  loss: 0.001159  eta: 0h1m  tot: 0h9m6s  (89.6%)47.9%  lr: 0.005510  loss: 0.001160  eta: 0h1m  tot: 0h9m6s  (89.6%)48.3%  lr: 0.005480  loss: 0.001162  eta: 0h1m  tot: 0h9m6s  (89.7%)48.8%  lr: 0.005450  loss: 0.001162  eta: <1min   tot: 0h9m7s  (89.8%)49.0%  lr: 0.005430  loss: 0.001160  eta: <1min   tot: 0h9m7s  (89.8%)49.0%  lr: 0.005420  loss: 0.001161  eta: <1min   tot: 0h9m7s  (89.8%)49.9%  lr: 0.005380  loss: 0.001163  eta: <1min   tot: 0h9m8s  (90.0%)49.9%  lr: 0.005380  loss: 0.001162  eta: <1min   tot: 0h9m8s  (90.0%)51.2%  lr: 0.005280  loss: 0.001174  eta: <1min   tot: 0h9m10s  (90.2%)52.5%  lr: 0.005180  loss: 0.001171  eta: <1min   tot: 0h9m11s  (90.5%)53.3%  lr: 0.005090  loss: 0.001171  eta: <1min   tot: 0h9m12s  (90.7%)53.7%  lr: 0.005030  loss: 0.001171  eta: <1min   tot: 0h9m12s  (90.7%)54.5%  lr: 0.004950  loss: 0.001174  eta: <1min   tot: 0h9m13s  (90.9%)55.7%  lr: 0.004840  loss: 0.001171  eta: <1min   tot: 0h9m14s  (91.1%)56.2%  lr: 0.004760  loss: 0.001171  eta: <1min   tot: 0h9m15s  (91.2%)56.4%  lr: 0.004750  loss: 0.001172  eta: <1min   tot: 0h9m15s  (91.3%)56.7%  lr: 0.004710  loss: 0.001172  eta: <1min   tot: 0h9m15s  (91.3%)57.7%  lr: 0.004550  loss: 0.001168  eta: <1min   tot: 0h9m17s  (91.5%)58.0%  lr: 0.004500  loss: 0.001168  eta: <1min   tot: 0h9m17s  (91.6%)58.4%  lr: 0.004440  loss: 0.001166  eta: <1min   tot: 0h9m18s  (91.7%)58.7%  lr: 0.004420  loss: 0.001166  eta: <1min   tot: 0h9m18s  (91.7%)59.0%  lr: 0.004370  loss: 0.001164  eta: <1min   tot: 0h9m18s  (91.8%)59.4%  lr: 0.004280  loss: 0.001164  eta: <1min   tot: 0h9m19s  (91.9%)59.8%  lr: 0.004230  loss: 0.001165  eta: <1min   tot: 0h9m19s  (92.0%)60.0%  lr: 0.004200  loss: 0.001166  eta: <1min   tot: 0h9m19s  (92.0%)60.4%  lr: 0.004100  loss: 0.001166  eta: <1min   tot: 0h9m20s  (92.1%)60.5%  lr: 0.004100  loss: 0.001166  eta: <1min   tot: 0h9m20s  (92.1%)60.7%  lr: 0.004070  loss: 0.001166  eta: <1min   tot: 0h9m20s  (92.1%)61.1%  lr: 0.004040  loss: 0.001163  eta: <1min   tot: 0h9m20s  (92.2%)%  lr: 0.003800  loss: 0.001160  eta: <1min   tot: 0h9m23s  (92.6%)Epoch: 100.0%  lr: 0.000180  loss: 0.001164  eta: <1min   tot: 0h10m5s  (100.0%).7%  lr: 0.003750  loss: 0.001163  eta: <1min   tot: 0h9m23s  (92.7%)65.0%  lr: 0.003630  loss: 0.001168  eta: <1min   tot: 0h9m25s  (93.0%)65.6%  lr: 0.003550  loss: 0.001167  eta: <1min   tot: 0h9m26s  (93.1%)  lr: 0.003470  loss: 0.001168  eta: <1min   tot: 0h9m27s  (93.4%)67.0%  lr: 0.003450  loss: 0.001169  eta: <1min   tot: 0h9m27s  (93.4%)67.7%  lr: 0.003370  loss: 0.001170  eta: <1min   tot: 0h9m28s  (93.5%)%  lr: 0.003230  loss: 0.001180  eta: <1min   tot: 0h9m29s  (93.7%)68.7%  lr: 0.003220  loss: 0.001183  eta: <1min   tot: 0h9m29s  (93.7%)68.8%  lr: 0.003200  loss: 0.001184  eta: <1min   tot: 0h9m30s  (93.8%)68.9%  lr: 0.003200  loss: 0.001184  eta: <1min   tot: 0h9m30s  (93.8%)69.2%  lr: 0.003170  loss: 0.001185  eta: <1min   tot: 0h9m30s  (93.8%)69.6%  lr: 0.003130  loss: 0.001187  eta: <1min   tot: 0h9m30s  (93.9%)70.2%  lr: 0.003070  loss: 0.001187  eta: <1min   tot: 0h9m31s  (94.0%)70.6%  lr: 0.003020  loss: 0.001184  eta: <1min   tot: 0h9m32s  (94.1%)71.0%  lr: 0.002990  loss: 0.001183  eta: <1min   tot: 0h9m32s  (94.2%)71.4%  lr: 0.002980  loss: 0.001181  eta: <1min   tot: 0h9m32s  (94.3%)  tot: 0h9m33s  (94.3%)71.8%  lr: 0.002950  loss: 0.001180  eta: <1min   tot: 0h9m33s  (94.4%)71.9%  lr: 0.002940  loss: 0.001182  eta: <1min   tot: 0h9m33s  (94.4%)72.0%  lr: 0.002940  loss: 0.001184  eta: <1min   tot: 0h9m33s  (94.4%)72.8%  lr: 0.002880  loss: 0.001184  eta: <1min   tot: 0h9m34s  (94.6%)73.6%  lr: 0.002780  loss: 0.001185  eta: <1min   tot: 0h9m35s  (94.7%)74.0%  lr: 0.002750  loss: 0.001185  eta: <1min   tot: 0h9m36s  (94.8%)h9m36s  (94.8%)74.4%  lr: 0.002720  loss: 0.001186  eta: <1min   tot: 0h9m36s  (94.9%)74.5%  lr: 0.002720  loss: 0.001185  eta: <1min   tot: 0h9m36s  (94.9%)%  lr: 0.002640  loss: 0.001184  eta: <1min   tot: 0h9m37s  (95.0%)75.3%  lr: 0.002600  loss: 0.001183  eta: <1min   tot: 0h9m37s  (95.1%)75.4%  lr: 0.002600  loss: 0.001184  eta: <1min   tot: 0h9m37s  (95.1%)%)76.5%  lr: 0.002510  loss: 0.001185  eta: <1min   tot: 0h9m39s  (95.3%)76.6%  lr: 0.002500  loss: 0.001184  eta: <1min   tot: 0h9m39s  (95.3%)77.2%  lr: 0.002470  loss: 0.001184  eta: <1min   tot: 0h9m39s  (95.4%)77.8%  lr: 0.002400  loss: 0.001184  eta: <1min   tot: 0h9m40s  (95.6%)78.2%  lr: 0.002370  loss: 0.001183  eta: <1min   tot: 0h9m40s  (95.6%)79.6%  lr: 0.002220  loss: 0.001181  eta: <1min   tot: 0h9m42s  (95.9%)79.8%  lr: 0.002220  loss: 0.001181  eta: <1min   tot: 0h9m42s  (96.0%)80.3%  lr: 0.002170  loss: 0.001181  eta: <1min   tot: 0h9m43s  (96.1%)80.9%  lr: 0.002090  loss: 0.001179  eta: <1min   tot: 0h9m43s  (96.2%)81.0%  lr: 0.002090  loss: 0.001180  eta: <1min   tot: 0h9m43s  (96.2%)81.2%  lr: 0.002080  loss: 0.001180  eta: <1min   tot: 0h9m44s  (96.2%)81.8%  lr: 0.001990  loss: 0.001179  eta: <1min   tot: 0h9m44s  (96.4%)82.4%  lr: 0.001930  loss: 0.001177  eta: <1min   tot: 0h9m45s  (96.5%)45s  (96.5%)84.0%  lr: 0.001790  loss: 0.001171  eta: <1min   tot: 0h9m47s  (96.8%)84.1%  lr: 0.001770  loss: 0.001172  eta: <1min   tot: 0h9m47s  (96.8%)85.5%  lr: 0.001660  loss: 0.001173  eta: <1min   tot: 0h9m49s  (97.1%)85.8%  lr: 0.001640  loss: 0.001173  eta: <1min   tot: 0h9m49s  (97.2%)85.9%  lr: 0.001620  loss: 0.001174  eta: <1min   tot: 0h9m49s  (97.2%)86.4%  lr: 0.001540  loss: 0.001174  eta: <1min   tot: 0h9m50s  (97.3%)87.1%  lr: 0.001490  loss: 0.001174  eta: <1min   tot: 0h9m50s  (97.4%)87.4%  lr: 0.001450  loss: 0.001176  eta: <1min   tot: 0h9m51s  (97.5%)89.3%  lr: 0.001240  loss: 0.001173  eta: <1min   tot: 0h9m53s  (97.9%)89.4%  lr: 0.001240  loss: 0.001173  eta: <1min   tot: 0h9m53s  (97.9%)90.2%  lr: 0.001190  loss: 0.001175  eta: <1min   tot: 0h9m54s  (98.0%)91.4%  lr: 0.001010  loss: 0.001174  eta: <1min   tot: 0h9m55s  (98.3%)92.1%  lr: 0.000940  loss: 0.001175  eta: <1min   tot: 0h9m56s  (98.4%)92.2%  lr: 0.000930  loss: 0.001176  eta: <1min   tot: 0h9m56s  (98.4%)93.6%  lr: 0.000800  loss: 0.001172  eta: <1min   tot: 0h9m57s  (98.7%)94.6%  lr: 0.000680  loss: 0.001172  eta: <1min   tot: 0h9m59s  (98.9%)94.9%  lr: 0.000640  loss: 0.001171  eta: <1min   tot: 0h9m59s  (99.0%)95.0%  lr: 0.000620  loss: 0.001170  eta: <1min   tot: 0h9m59s  (99.0%)96.7%  lr: 0.000500  loss: 0.001167  eta: <1min   tot: 0h10m1s  (99.3%)97.1%  lr: 0.000480  loss: 0.001165  eta: <1min   tot: 0h10m1s  (99.4%)97.9%  lr: 0.000420  loss: 0.001165  eta: <1min   tot: 0h10m2s  (99.6%)98.1%  lr: 0.000390  loss: 0.001164  eta: <1min   tot: 0h10m2s  (99.6%)\n",
      " ---+++                Epoch    4 Train error : 0.00118168 +++--- ���\n",
      "Saving model to file : data/starspace\n",
      "Saving model in tsv format : data/starspace.tsv\n"
     ]
    }
   ],
   "source": [
    "######### TRAINING HAPPENING HERE #############\n",
    "!starspace train -trainMode 3 -adagrad True -ngrams 1 -epoch 5 -dim 100 -similarity cosine -verbose True -fileFormat 'labelDoc' -negSearchLimit 10 -lr 0.05 -trainFile 'data/train_prepared.tsv' -model 'data/starspace'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And now we can compare the new embeddings with the previous ones. You can find trained word vectors in the file *[model_file_name].tsv*. Upload the embeddings from StarSpace into a dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_count = sum(1 for line in open('data/starspace.tsv'))\n",
    "with open('data/starspace.tsv', 'r') as inp, open('data/word2vec-format.txt', 'w') as outp:\n",
    "    line_count = str(line_count)    # line count of the tsv file (as string)\n",
    "    dimensions = '100'    # vector size (as string)\n",
    "    outp.write(' '.join([line_count, dimensions]) + '\\n')\n",
    "    for line in inp:\n",
    "        words = line.strip().split()\n",
    "        outp.write(' '.join(words) + '\\n')\n",
    "starspace_embeddings = gensim.models.KeyedVectors.load_word2vec_format('data/word2vec-format.txt',binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prepared_validation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b5fb095b89e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mss_prepared_ranking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprepared_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrank_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarspace_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mss_prepared_ranking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mranks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prepared_validation' is not defined"
     ]
    }
   ],
   "source": [
    "ss_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ss_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(ss_prepared_ranking, k), \n",
    "                                               k, hits_count(ss_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to training for the particular task with the supervised data, you should expect to obtain a higher quality than for the previous approach. In additiion, despite the fact that StarSpace's trained vectors have a smaller dimension than word2vec's, it provides better results in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (StarSpaceRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates for trained representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "starspace_ranks_results = []\n",
    "prepared_test_data = 'data/test_prepared.tsv'\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    starspace_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('StarSpaceRanks', matrix_to_string(starspace_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, **don't remove** the file with these embeddings because you will need them in the final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authorization & Submission\n",
    "To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate token on this programming assignment page. <b>Note:</b> Token expires 30 minutes after generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STUDENT_EMAIL = 'daisysunmeng@gamil.com'\n",
    "STUDENT_TOKEN = # TOKEN \n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to submit these answers, run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
